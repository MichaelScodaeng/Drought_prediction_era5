{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8d0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added project root to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd42f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LEN = 12  # Number of input time steps\n",
    "FORECAST_LEN = 6  # Number of forecast time steps\n",
    "PATH = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "VARIABLES = [\n",
    "    'total_precipitation_6hr',\n",
    "    '2m_temperature', '2m_dewpoint_temperature', 'surface_pressure',\n",
    "    'mean_sea_level_pressure', '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "    '10m_wind_speed', 'u_component_of_wind', 'v_component_of_wind',\n",
    "    'total_column_water_vapour', 'integrated_vapor_transport', 'boundary_layer_height',\n",
    "    'specific_humidity', 'total_cloud_cover',\n",
    "    'mean_surface_net_short_wave_radiation_flux',\n",
    "    'mean_surface_latent_heat_flux', 'mean_surface_sensible_heat_flux',\n",
    "    'snow_depth', 'sea_surface_temperature', 'volumetric_soil_water_layer_1',\n",
    "    'mean_vertically_integrated_moisture_divergence', 'eddy_kinetic_energy',\n",
    "    'land_sea_mask'\n",
    "]\n",
    "TARGET_VARIABLE = 'total_precipitation_6hr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2284f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GriddedClimateDataset(Dataset):\n",
    "    def __init__(self, file_path, input_len=12, forecast_len=1, variables=None,\n",
    "                 target_variable=None, lat_bounds=(21.0, 5.0), lon_bounds=(97, 106),\n",
    "                 time_slice=slice(\"2022-01\", \"2022-02\")):\n",
    "        self.input_len = input_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.variables = variables\n",
    "        self.target_variable = target_variable\n",
    "\n",
    "        # Load dataset (Dask-compatible)\n",
    "        if file_path.endswith(\".nc\"):\n",
    "            self.ds = xr.open_dataset(file_path)\n",
    "        else:\n",
    "            self.ds = xr.open_zarr(file_path, consolidated=True, decode_times=False,\n",
    "                                   storage_options={\"token\": \"anon\", \"asynchronous\": False})\n",
    "            from cftime import num2date\n",
    "            units = self.ds.time.attrs.get(\"units\", \"hours since 1900-01-01 00:00:0.0\")\n",
    "            cal = self.ds.time.attrs.get(\"calendar\", \"standard\")\n",
    "            self.ds[\"time\"] = (\"time\", num2date(self.ds.time.values, units, calendar=cal))\n",
    "\n",
    "        # Subset time and region\n",
    "        self.ds = self.ds.sel(time=time_slice, latitude=slice(*lat_bounds))\n",
    "        self.ds = self.ds.where((self.ds.longitude >= lon_bounds[0]) | \n",
    "                                (self.ds.longitude <= lon_bounds[1]), drop=True)\n",
    "\n",
    "        # Prepare variable handles (do not load yet)\n",
    "        self.inputs = {}\n",
    "        for var in variables:\n",
    "            da = self.ds[var]\n",
    "            if \"level\" in da.dims:\n",
    "                da = da.mean(dim=\"level\")\n",
    "            if set(da.dims) == {\"latitude\", \"longitude\"}:\n",
    "                da = da.expand_dims(time=self.ds.time)\n",
    "            self.inputs[var] = da.chunk({'time': -1})  # Lazy chunking\n",
    "\n",
    "        self.target = self.ds[target_variable]\n",
    "        if \"level\" in self.target.dims:\n",
    "            self.target = self.target.mean(dim=\"level\")\n",
    "        self.target = self.target.chunk({'time': -1})\n",
    "\n",
    "        # Determine sequence count\n",
    "        self.length = len(self.ds.time) - input_len - forecast_len + 1\n",
    "        print(f\"[Dataset] Initialized with {self.length} samples.\")\n",
    "        print(f\"[Dataset] Estimated size: {self.ds.nbytes / 1e6:.2f} MB\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        t0 = idx\n",
    "        t1 = idx + self.input_len\n",
    "        tf = t1 + self.forecast_len\n",
    "\n",
    "        # Stack variables: [C, T, H, W]\n",
    "        x_vars = []\n",
    "        for var in self.variables:\n",
    "            da = self.inputs[var].isel(time=slice(t0, t1))  # [T, H, W]\n",
    "            x = da.transpose(\"time\", \"latitude\", \"longitude\").values  # triggers lazy load\n",
    "            x_vars.append(x)\n",
    "\n",
    "        x_array = np.stack(x_vars, axis=0)  # shape: [C, T, H, W]\n",
    "\n",
    "        # Target: [forecast_len, H, W]\n",
    "        y = self.target.isel(time=slice(t1, tf)).transpose(\"time\", \"latitude\", \"longitude\").values\n",
    "\n",
    "        return x_array, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0cb83f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.1 is exactly one major version older than the runtime version 6.31.1 at api.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Initialized with 93527 samples.\n",
      "[Dataset] Estimated size: 8020243.68 MB\n"
     ]
    }
   ],
   "source": [
    "dataset = GriddedClimateDataset(PATH, input_len=INPUT_LEN, forecast_len=FORECAST_LEN, \n",
    "                                  variables=VARIABLES, target_variable=TARGET_VARIABLE,time_slice=slice(\"1959\", \"2023\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d336595b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataset.ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d49277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating min/max for scaling:   0%|          | 0/24 [03:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(da\u001b[38;5;241m.\u001b[39mdims) \u001b[38;5;241m==\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m     12\u001b[0m         da \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mexpand_dims(time\u001b[38;5;241m=\u001b[39mds\u001b[38;5;241m.\u001b[39mtime)  \u001b[38;5;66;03m# Broadcast static vars\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     mins\u001b[38;5;241m.\u001b[39mappend(\u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     14\u001b[0m     maxs\u001b[38;5;241m.\u001b[39mappend(da\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mcompute())\n\u001b[0;32m     16\u001b[0m x_data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mfloat\u001b[39m(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m mins])\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\dataarray.py:1202\u001b[0m, in \u001b[0;36mDataArray.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;124;03mremote source into memory and return a new array.\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\dataarray.py:1170\u001b[0m, in \u001b[0;36mDataArray.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\dataset.py:870\u001b[0m, in \u001b[0;36mDataset.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    869\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:86\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[1;34m(self, *data, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m     83\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\dask\\base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "input_vars = VARIABLES # ← your config['variables']\n",
    "mins, maxs = [], []\n",
    "from tqdm import tqdm\n",
    "for var in tqdm(input_vars, desc=\"Calculating min/max for scaling\"):\n",
    "    da = ds[var]\n",
    "    if \"level\" in da.dims:\n",
    "        da = da.mean(dim=\"level\")  # Collapse level\n",
    "    if set(da.dims) == {\"latitude\", \"longitude\"}:\n",
    "        da = da.expand_dims(time=ds.time)  # Broadcast static vars\n",
    "    mins.append(da.min().compute())\n",
    "    maxs.append(da.max().compute())\n",
    "\n",
    "x_data_min = np.array([float(m) for m in mins])\n",
    "x_data_max = np.array([float(m) for m in maxs])\n",
    "\n",
    "scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_x.min_ = -x_data_min * (2.0 / (x_data_max - x_data_min + 1e-8))\n",
    "scaler_x.scale_ = 2.0 / (x_data_max - x_data_min + 1e-8)\n",
    "scaler_x.data_min_ = x_data_min\n",
    "scaler_x.data_max_ = x_data_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b98fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = TARGET_VARIABLE  # or your config['target_variable']\n",
    "target = ds[target_var]\n",
    "if \"level\" in target.dims:\n",
    "    target = target.mean(dim=\"level\")\n",
    "target_min = float(target.min().compute())\n",
    "target_max = float(target.max().compute())\n",
    "\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y.min_ = -target_min * (2.0 / (target_max - target_min + 1e-8))\n",
    "scaler_y.scale_ = 2.0 / (target_max - target_min + 1e-8)\n",
    "scaler_y.data_min_ = np.array([target_min])\n",
    "scaler_y.data_max_ = np.array([target_max])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
