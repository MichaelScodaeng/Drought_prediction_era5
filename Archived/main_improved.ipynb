{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97051770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb content to be used in Jupyter Notebook format\n",
    "\n",
    "# Section 1: Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from src.adm_prednet.stacked_model import ADMStackedModel\n",
    "from src.adm_prednet.pe_utils import add_temporal_pe, add_spatial_pe\n",
    "from src.adm_prednet.masked_loss import masked_mse\n",
    "from src.adm_prednet.evaluate import evaluate_model\n",
    "from src.adm_prednet.save_predictions import save_predictions_to_csv\n",
    "from src.adm_prednet.visualize_utils import plot_comparison\n",
    "from src.grid_utils import create_gridded_data\n",
    "from src.data_utils import split_data_chronologically\n",
    "# Section 4: Dataset Class from Grid\n",
    "from torch.utils.data import Dataset\n",
    "class GriddedSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, gridded_tensor, input_steps=12, target_steps=1, target_indices=[0, 1]):\n",
    "        self.data = torch.tensor(gridded_tensor, dtype=torch.float32)\n",
    "        self.input_steps = input_steps\n",
    "        self.target_steps = target_steps\n",
    "        self.target_indices = target_indices\n",
    "        self.grid_shape = self.data.shape[1:3]  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.input_steps - self.target_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx:idx + self.input_steps]  # [T, H, W, C]\n",
    "        X = X.permute(0, 3, 1, 2).float()           # → [T, C, H, W]\n",
    "        Y = self.data[idx + self.input_steps]      # [H, W, C]\n",
    "        Y = Y[..., self.target_indices].permute(2, 0, 1)  # [C_target, H, W]\n",
    "        return X, Y\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887095e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.1 is exactly one major version older than the runtime version 6.31.1 at api.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import optuna\n",
    "import os\n",
    "from src.adm_prednet.train import train_adm_model\n",
    "from src.adm_prednet.stacked_model import ADMStackedModel\n",
    "from src.adm_prednet.evaluate import evaluate_model\n",
    "from src.adm_prednet.save_predictions import save_predictions_to_csv\n",
    "from pathlib import Path\n",
    "from src.data_utils import split_data_chronologically\n",
    "from src.grid_utils import create_gridded_data\n",
    "class GriddedSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, gridded_tensor, input_steps=12, target_steps=1, target_indices=[0, 1]):\n",
    "        self.data = torch.tensor(gridded_tensor, dtype=torch.float32)\n",
    "        self.input_steps = input_steps\n",
    "        self.target_steps = target_steps\n",
    "        self.target_indices = target_indices\n",
    "        self.grid_shape = self.data.shape[1:3]  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.input_steps - self.target_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx:idx + self.input_steps]  # [T, H, W, C]\n",
    "        X = X.permute(0, 3, 1, 2).float()           # → [T, C, H, W]\n",
    "        Y = self.data[idx + self.input_steps]      # [H, W, C]\n",
    "        Y = Y[..., self.target_indices].permute(2, 0, 1)  # [C_target, H, W]\n",
    "        return X, Y\n",
    "\n",
    "class ADMForecastingPipeline:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = yaml.safe_load(open(config_path))\n",
    "        self.model = None\n",
    "        self.land_mask = None\n",
    "        self.raw_df = None\n",
    "        self.study = None\n",
    "        self.best_params = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.full_df = pd.read_csv(self.config[\"data\"][\"csv_path\"])\n",
    "        self.raw_df = self.full_df  # For saving predictions\n",
    "\n",
    "        self.train_df, self.val_df, self.test_df = split_data_chronologically(self.full_df, self.config)\n",
    "        self.gridded_tensor_train, self.land_mask = create_gridded_data(self.train_df, self.config)  # assume same shape for all\n",
    "        self.gridded_tensor_val, self.land_mask = create_gridded_data(self.val_df, self.config)  # assume same shape for all\n",
    "        self.gridded_tensor_test, self.land_mask = create_gridded_data(self.test_df, self.config)  # assume same shape for all\n",
    "        self.gridded_tensor_full, self.land_mask = create_gridded_data(self.full_df, self.config)  # assume same shape for all\n",
    "        self.train_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_train,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.val_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_val,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.test_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_test,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.full_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_full,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.land_mask =  torch.tensor(self.land_mask, dtype=torch.float32).cuda()\n",
    "        Path(self.config[\"output\"][\"predictions_csv\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = ADMStackedModel(\n",
    "            input_channels=self.config[\"model\"][\"input_channels\"],\n",
    "            hidden_channels=self.config[\"model\"][\"hidden_channels\"],\n",
    "            n_layers=self.config[\"model\"][\"n_layers\"],\n",
    "            output_targets=self.config[\"model\"][\"output_targets\"]\n",
    "        ).cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.study = optuna.create_study(direction=\"minimize\")\n",
    "        self.study.optimize(lambda trial: self.objective(trial, self.config, self.train_dataset, self.val_dataset, self.land_mask), n_trials=10)\n",
    "\n",
    "        self.best_params = self.study.best_trial.params\n",
    "        print(\"Best hyperparameters found by Optuna:\", self.best_params)\n",
    "\n",
    "        # Reconstruct hidden_channels\n",
    "        n_layers = self.best_params['n_layers']\n",
    "        hidden_channels = [self.best_params[f'hidden_channels_{i}'] for i in range(n_layers)]\n",
    "        print(\"Reconstructed hidden_channels:\", hidden_channels)\n",
    "\n",
    "        cfg_final = self.config.copy()\n",
    "        cfg_final['model']['hidden_channels'] = hidden_channels\n",
    "        cfg_final['model']['n_layers'] = n_layers\n",
    "        cfg_final['training']['learning_rate'] = self.best_params['learning_rate']\n",
    "        cfg_final['training']['dropout_rate'] = self.best_params['dropout_rate']\n",
    "        cfg_final['training']['batch_size'] = self.best_params['batch_size']\n",
    "\n",
    "        self.train_val_dataset = ConcatDataset([self.train_dataset, self.val_dataset])\n",
    "        train_val_loader = DataLoader(self.train_val_dataset, batch_size=cfg_final['training']['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=cfg_final['training']['batch_size'], shuffle=False)\n",
    "\n",
    "        self.model = train_adm_model(train_val_loader, cfg_final, \n",
    "        val_loader=test_loader, land_mask=self.land_mask,log_path=self.config[\"output\"][\"predictions_csv\"]+\"prediction_log.csv\")\n",
    "        torch.save(self.model.state_dict(), \"final_model.pth\")\n",
    "\n",
    "        # Final evaluation\n",
    "        metrics = evaluate_model(self.model, test_loader, self.land_mask, output_targets=cfg_final['model']['output_targets'])\n",
    "        print(\"Final Evaluation on Test Set:\")\n",
    "        for target, vals in metrics.items():\n",
    "            print(f\"  {target}: {vals}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "        metrics = evaluate_model(self.model, val_loader, self.land_mask, self.config[\"model\"][\"output_targets\"])\n",
    "        print(\"Evaluation Metrics on Validation Set:\")\n",
    "        for target, vals in metrics.items():\n",
    "            print(f\"  {target}: {vals}\")\n",
    "        return metrics\n",
    "\n",
    "    def predict_and_save(self):\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "        save_predictions_to_csv(\n",
    "            self.model,\n",
    "            val_loader,\n",
    "            self.land_mask,\n",
    "            self.config[\"model\"][\"output_targets\"],\n",
    "            self.raw_df,\n",
    "            self.config[\"output\"][\"predictions_csv\"]\n",
    "        )\n",
    "\n",
    "    def objective(self, trial, config, train_dataset, val_dataset, land_mask):\n",
    "\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 5)\n",
    "        hidden_channels = [trial.suggest_categorical(f'hidden_channels_{i}', [128, 256,512]) for i in range(n_layers)]\n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 5e-3, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3)\n",
    "        batch_size = trial.suggest_int('batch_size', 8, 32, step=8)\n",
    "\n",
    "        config['model']['hidden_channels'] = hidden_channels\n",
    "        config['model']['n_layers'] = n_layers\n",
    "        config['training']['learning_rate'] = learning_rate\n",
    "        config['training']['dropout_rate'] = dropout_rate\n",
    "        config['training']['batch_size'] = batch_size\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = train_adm_model(train_loader, config, val_loader=val_loader, land_mask=land_mask)\n",
    "        metrics = evaluate_model(model, val_loader, land_mask, output_targets=config['model']['output_targets'])\n",
    "        ret_error = []\n",
    "        for k,v in metrics.items():\n",
    "            ret_error.append(v['RMSE'])\n",
    "        return sum(ret_error) / len(ret_error)\n",
    "\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        self.train()\n",
    "        self.evaluate()\n",
    "        self.predict_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0adc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: Train ends 2017-12-31 00:00:00, Validation ends 2020-12-31 00:00:00\n",
      "Warning: Time column 'time' is not datetime. Attempting conversion.\n",
      "Train set shape: (251316, 19), Time range: 1901-01-16 00:00:00 to 2017-12-16 00:00:00\n",
      "Validation set shape: (6444, 19), Time range: 2018-01-16 00:00:00 to 2020-12-16 00:00:00\n",
      "Test set shape: (6444, 19), Time range: 2021-01-16 00:00:00 to 2023-12-16 00:00:00\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (1404, 29, 16, 15)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Gridding Process Finished ---\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (36, 29, 16, 15)...\n",
      "--- Data Gridding Process Finished ---\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (36, 29, 16, 15)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Gridding Process Finished ---\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (1476, 29, 16, 15)...\n",
      "--- Data Gridding Process Finished ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 01:01:47,528] A new study created in memory with name: no-name-d523ff0c-84ff-4ce5-ac5a-53ed929bd37e\n",
      "[W 2025-06-17 01:02:12,864] Trial 0 failed with parameters: {'n_layers': 2, 'hidden_channels_0': 128, 'hidden_channels_1': 64, 'learning_rate': 4.4744793387882014e-05, 'dropout_rate': 0.22261005343030205, 'batch_size': 16} because of the following error: RuntimeError('Parent directory checkpoints/adm_prednet/predictions_prepet does not exist.').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\peera\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_48624\\2952359607.py\", line 87, in <lambda>\n",
      "    self.study.optimize(lambda trial: self.objective(trial, self.config, self.train_dataset, self.val_dataset, self.land_mask), n_trials=1)\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_48624\\2952359607.py\", line 154, in objective\n",
      "    model = train_adm_model(train_loader, config, val_loader=val_loader, land_mask=land_mask)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\adm_prednet\\train.py\", line 111, in train_adm_model\n",
      "    torch.save(model.state_dict(), config['output']['save_model_path'])\n",
      "  File \"c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\serialization.py\", line 964, in save\n",
      "    with _open_zipfile_writer(f) as opened_zipfile:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\serialization.py\", line 828, in _open_zipfile_writer\n",
      "    return container(name_or_buffer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\serialization.py\", line 792, in __init__\n",
      "    torch._C.PyTorchFileWriter(\n",
      "RuntimeError: Parent directory checkpoints/adm_prednet/predictions_prepet does not exist.\n",
      "[W 2025-06-17 01:02:12,871] Trial 0 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.1619\n",
      "Epoch 1 - Validation Loss: 0.0655\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory checkpoints/adm_prednet/predictions_prepet does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pipeline_prepet \u001b[38;5;241m=\u001b[39m ADMForecastingPipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_prepet.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpipeline_prepet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 163\u001b[0m, in \u001b[0;36mADMForecastingPipeline.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_data()\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_and_save()\n",
      "Cell \u001b[1;32mIn[2], line 87\u001b[0m, in \u001b[0;36mADMForecastingPipeline.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mland_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found by Optuna:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[2], line 87\u001b[0m, in \u001b[0;36mADMForecastingPipeline.train.<locals>.<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mland_mask\u001b[49m\u001b[43m)\u001b[49m, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found by Optuna:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "Cell \u001b[1;32mIn[2], line 154\u001b[0m, in \u001b[0;36mADMForecastingPipeline.objective\u001b[1;34m(self, trial, config, train_dataset, val_dataset, land_mask)\u001b[0m\n\u001b[0;32m    151\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 154\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_adm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mland_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mland_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m metrics \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, land_mask, output_targets\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_targets\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    156\u001b[0m ret_error \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\adm_prednet\\train.py:111\u001b[0m, in \u001b[0;36mtrain_adm_model\u001b[1;34m(dataset, config, val_loader, land_mask, log_path)\u001b[0m\n\u001b[0;32m    109\u001b[0m     best_val_loss \u001b[38;5;241m=\u001b[39m avg_val_loss\n\u001b[0;32m    110\u001b[0m     epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 111\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_model_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     best_model_state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\serialization.py:964\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    961\u001b[0m     f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(f)\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    965\u001b[0m         _save(\n\u001b[0;32m    966\u001b[0m             obj,\n\u001b[0;32m    967\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    970\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    971\u001b[0m         )\n\u001b[0;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\serialization.py:828\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\serialization.py:792\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    786\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[0;32m    788\u001b[0m         )\n\u001b[0;32m    789\u001b[0m     )\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m--> 792\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory checkpoints/adm_prednet/predictions_prepet does not exist."
     ]
    }
   ],
   "source": [
    "pipeline_prepet = ADMForecastingPipeline(\"config_prepet.yaml\")\n",
    "pipeline_prepet.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b6972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: Train ends 2017-12-31 00:00:00, Validation ends 2020-12-31 00:00:00\n",
      "Warning: Time column 'time' is not datetime. Attempting conversion.\n",
      "Train set shape: (251316, 19), Time range: 1901-01-16 00:00:00 to 2017-12-16 00:00:00\n",
      "Validation set shape: (6444, 19), Time range: 2018-01-16 00:00:00 to 2020-12-16 00:00:00\n",
      "Test set shape: (6444, 19), Time range: 2021-01-16 00:00:00 to 2023-12-16 00:00:00\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (1404, 29, 16, 16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Gridding Process Finished ---\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (36, 29, 16, 16)...\n",
      "--- Data Gridding Process Finished ---\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (36, 29, 16, 16)...\n",
      "--- Data Gridding Process Finished ---\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (1476, 29, 16, 16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 01:05:51,982] A new study created in memory with name: no-name-f90d33fe-2bb0-45a7-b73f-197c23ebb44f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Gridding Process Finished ---\n",
      "Epoch 1 - Train Loss: 0.1475\n",
      "Epoch 1 - Validation Loss: 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-17 01:06:19,097] Trial 0 finished with value: 0.26934635639190674 and parameters: {'n_layers': 2, 'hidden_channels_0': 128, 'hidden_channels_1': 128, 'learning_rate': 7.719043349275802e-05, 'dropout_rate': 0.29505529941179803, 'batch_size': 8}. Best is trial 0 with value: 0.26934635639190674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found by Optuna: {'n_layers': 2, 'hidden_channels_0': 128, 'hidden_channels_1': 128, 'learning_rate': 7.719043349275802e-05, 'dropout_rate': 0.29505529941179803, 'batch_size': 8}\n",
      "Reconstructed hidden_channels: [128, 128]\n",
      "Epoch 1 - Train Loss: 0.1396\n",
      "Epoch 1 - Validation Loss: 0.0505\n",
      "Training log saved to run_outputs/adm_prednet/predictions_all/prediction_log.csv\n",
      "Final Evaluation on Test Set:\n",
      "  PET: {'RMSE': np.float32(0.3586714), 'MAE': np.float32(0.2858248), 'R2': 0.7818723917007446}\n",
      "  PRE: {'RMSE': np.float32(0.2398288), 'MAE': np.float32(0.18879706), 'R2': 0.805541455745697}\n",
      "  SPEI: {'RMSE': np.float32(0.14843479), 'MAE': np.float32(0.11828766), 'R2': 0.8967832326889038}\n",
      "Evaluation Metrics on Validation Set:\n",
      "  PET: {'RMSE': np.float32(0.3254588), 'MAE': np.float32(0.26095405), 'R2': 0.8202466368675232}\n",
      "  PRE: {'RMSE': np.float32(0.26462525), 'MAE': np.float32(0.20814984), 'R2': 0.8123681545257568}\n",
      "  SPEI: {'RMSE': np.float32(0.17962982), 'MAE': np.float32(0.13916011), 'R2': 0.8876003623008728}\n",
      "Saved PET predictions to: run_outputs/adm_prednet/predictions_all/PET_predictions.csv\n",
      "Saved PRE predictions to: run_outputs/adm_prednet/predictions_all/PRE_predictions.csv\n",
      "Saved SPEI predictions to: run_outputs/adm_prednet/predictions_all/SPEI_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "pipeline_all = ADMForecastingPipeline(\"config_prepetspei.yaml\")\n",
    "pipeline_all.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
