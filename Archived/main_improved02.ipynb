{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97051770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb content to be used in Jupyter Notebook format\n",
    "\n",
    "# Section 1: Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from src.adm_prednet.stacked_model import ADMStackedModel\n",
    "from src.adm_prednet.pe_utils import add_temporal_pe, add_spatial_pe\n",
    "from src.adm_prednet.masked_loss import masked_mse\n",
    "from src.adm_prednet.evaluate import evaluate_model\n",
    "from src.adm_prednet.save_predictions import save_predictions_to_csv\n",
    "from src.adm_prednet.visualize_utils import plot_comparison\n",
    "from src.grid_utils import create_gridded_data\n",
    "from src.data_utils import split_data_chronologically\n",
    "# Section 4: Dataset Class from Grid\n",
    "from torch.utils.data import Dataset\n",
    "class GriddedSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, gridded_tensor, input_steps=12, target_steps=1, target_indices=[0, 1]):\n",
    "        self.data = torch.tensor(gridded_tensor, dtype=torch.float32)\n",
    "        self.input_steps = input_steps\n",
    "        self.target_steps = target_steps\n",
    "        self.target_indices = target_indices\n",
    "        self.grid_shape = self.data.shape[1:3]  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.input_steps - self.target_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx:idx + self.input_steps]  # [T, H, W, C]\n",
    "        X = X.permute(0, 3, 1, 2).float()           # → [T, C, H, W]\n",
    "        Y = self.data[idx + self.input_steps]      # [H, W, C]\n",
    "        Y = Y[..., self.target_indices].permute(2, 0, 1)  # [C_target, H, W]\n",
    "        return X, Y\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887095e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "import optuna\n",
    "import os\n",
    "from src.adm_prednet.train import train_adm_model\n",
    "from src.adm_prednet.stacked_model import ADMStackedModel\n",
    "from src.adm_prednet.evaluate import evaluate_model\n",
    "from src.adm_prednet.save_predictions import save_predictions_to_csv\n",
    "from pathlib import Path\n",
    "from src.data_utils import split_data_chronologically\n",
    "from src.grid_utils import create_gridded_data\n",
    "class GriddedSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, gridded_tensor, input_steps=12, target_steps=1, target_indices=[0, 1]):\n",
    "        self.data = torch.tensor(gridded_tensor, dtype=torch.float32)\n",
    "        self.input_steps = input_steps\n",
    "        self.target_steps = target_steps\n",
    "        self.target_indices = target_indices\n",
    "        self.grid_shape = self.data.shape[1:3]  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.input_steps - self.target_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx:idx + self.input_steps]  # [T, H, W, C]\n",
    "        X = X.permute(0, 3, 1, 2).float()           # → [T, C, H, W]\n",
    "        Y = self.data[idx + self.input_steps]      # [H, W, C]\n",
    "        Y = Y[..., self.target_indices].permute(2, 0, 1)  # [C_target, H, W]\n",
    "        return X, Y\n",
    "\n",
    "class ADMForecastingPipeline:\n",
    "    def __init__(self, config_path):\n",
    "        self.config = yaml.safe_load(open(config_path))\n",
    "        self.model = None\n",
    "        self.land_mask = None\n",
    "        self.raw_df = None\n",
    "        self.study = None\n",
    "        self.best_params = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.full_df = pd.read_csv(self.config[\"data\"][\"csv_path\"])\n",
    "        self.raw_df = self.full_df  # For saving predictions\n",
    "\n",
    "        self.train_df, self.val_df, self.test_df = split_data_chronologically(self.full_df, self.config)\n",
    "        self.gridded_tensor_train, self.land_mask = create_gridded_data(self.train_df, self.config)  # assume same shape for all\n",
    "        self.gridded_tensor_val, self.land_mask = create_gridded_data(self.val_df, self.config)  # assume same shape for all\n",
    "        self.gridded_tensor_test, self.land_mask = create_gridded_data(self.test_df, self.config)  # assume same shape for all\n",
    "        self.gridded_tensor_full, self.land_mask = create_gridded_data(self.full_df, self.config)  # assume same shape for all\n",
    "        self.train_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_train,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.val_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_val,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.test_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_test,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.full_dataset = GriddedSeq2SeqDataset(\n",
    "            self.gridded_tensor_full,\n",
    "            input_steps=self.config['training']['input_steps'],\n",
    "            target_steps=self.config[\"training\"][\"output_steps\"],\n",
    "            target_indices=list(range(len(self.config[\"model\"][\"output_targets\"])))\n",
    "        )\n",
    "        self.land_mask =  torch.tensor(self.land_mask, dtype=torch.float32).cuda()\n",
    "        Path(self.config[\"output\"][\"predictions_csv\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = ADMStackedModel(\n",
    "            input_channels=self.config[\"model\"][\"input_channels\"],\n",
    "            hidden_channels=self.config[\"model\"][\"hidden_channels\"],\n",
    "            n_layers=self.config[\"model\"][\"n_layers\"],\n",
    "            output_targets=self.config[\"model\"][\"output_targets\"]\n",
    "        ).cuda()\n",
    "\n",
    "    def train(self):\n",
    "        self.study = optuna.create_study(direction=\"minimize\")\n",
    "        self.study.optimize(lambda trial: self.objective(trial, self.config, self.train_dataset, self.val_dataset, self.land_mask), n_trials=10)\n",
    "\n",
    "        self.best_params = self.study.best_trial.params\n",
    "        print(\"Best hyperparameters found by Optuna:\", self.best_params)\n",
    "\n",
    "        # Reconstruct hidden_channels\n",
    "        n_layers = self.best_params['n_layers']\n",
    "        hidden_channels = [self.best_params[f'hidden_channels_{i}'] for i in range(n_layers)]\n",
    "        print(\"Reconstructed hidden_channels:\", hidden_channels)\n",
    "\n",
    "        cfg_final = self.config.copy()\n",
    "        cfg_final['model']['hidden_channels'] = hidden_channels\n",
    "        cfg_final['model']['n_layers'] = n_layers\n",
    "        cfg_final['training']['learning_rate'] = self.best_params['learning_rate']\n",
    "        cfg_final['training']['dropout_rate'] = self.best_params['dropout_rate']\n",
    "        cfg_final['training']['batch_size'] = self.best_params['batch_size']\n",
    "\n",
    "        self.train_val_dataset = ConcatDataset([self.train_dataset, self.val_dataset])\n",
    "        train_val_loader = DataLoader(self.train_val_dataset, batch_size=cfg_final['training']['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=cfg_final['training']['batch_size'], shuffle=False)\n",
    "\n",
    "        self.model = train_adm_model(train_val_loader, cfg_final, \n",
    "        val_loader=test_loader, land_mask=self.land_mask,log_path=self.config[\"output\"][\"predictions_csv\"]+\"prediction_log.csv\")\n",
    "        torch.save(self.model.state_dict(), \"final_model.pth\")\n",
    "\n",
    "        # Final evaluation\n",
    "        metrics = evaluate_model(self.model, test_loader, self.land_mask, output_targets=cfg_final['model']['output_targets'])\n",
    "        print(\"Final Evaluation on Test Set:\")\n",
    "        for target, vals in metrics.items():\n",
    "            print(f\"  {target}: {vals}\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "        metrics = evaluate_model(self.model, val_loader, self.land_mask, self.config[\"model\"][\"output_targets\"])\n",
    "        print(\"Evaluation Metrics on Validation Set:\")\n",
    "        for target, vals in metrics.items():\n",
    "            print(f\"  {target}: {vals}\")\n",
    "        return metrics\n",
    "\n",
    "    def predict_and_save(self):\n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "        save_predictions_to_csv(\n",
    "            self.model,\n",
    "            val_loader,\n",
    "            self.land_mask,\n",
    "            self.config[\"model\"][\"output_targets\"],\n",
    "            self.raw_df,\n",
    "            self.config[\"output\"][\"predictions_csv\"]\n",
    "        )\n",
    "\n",
    "    def objective(self, trial, config, train_dataset, val_dataset, land_mask):\n",
    "\n",
    "        n_layers = trial.suggest_int('n_layers', 2, 5)\n",
    "        hidden_channels = [trial.suggest_categorical(f'hidden_channels_{i}', [128, 256,512]) for i in range(n_layers)]\n",
    "        learning_rate = trial.suggest_float('learning_rate', 2e-5, 5e-3, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3)\n",
    "        batch_size = trial.suggest_int('batch_size', 8, 32, step=8)\n",
    "\n",
    "        config['model']['hidden_channels'] = hidden_channels\n",
    "        config['model']['n_layers'] = n_layers\n",
    "        config['training']['learning_rate'] = learning_rate\n",
    "        config['training']['dropout_rate'] = dropout_rate\n",
    "        config['training']['batch_size'] = batch_size\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = train_adm_model(train_loader, config, val_loader=val_loader, land_mask=land_mask)\n",
    "        metrics = evaluate_model(model, val_loader, land_mask, output_targets=config['model']['output_targets'])\n",
    "        ret_error = []\n",
    "        for k,v in metrics.items():\n",
    "            ret_error.append(v['RMSE'])\n",
    "        return sum(ret_error) / len(ret_error)\n",
    "\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        self.train()\n",
    "        self.evaluate()\n",
    "        self.predict_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pre = ADMForecastingPipeline(\"config_pre.yaml\")\n",
    "pipeline_pre.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_pet = ADMForecastingPipeline(\"config_pet.yaml\")\n",
    "pipeline_pet.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_spei = ADMForecastingPipeline(\"config_spei.yaml\")\n",
    "pipeline_spei.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
