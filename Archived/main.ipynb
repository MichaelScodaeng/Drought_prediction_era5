{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57efd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: Train ends 2017-12-31 00:00:00, Validation ends 2020-12-31 00:00:00\n",
      "Warning: Time column 'time' is not datetime. Attempting conversion.\n",
      "Train set shape: (251316, 19), Time range: 1901-01-16 00:00:00 to 2017-12-16 00:00:00\n",
      "Validation set shape: (6444, 19), Time range: 2018-01-16 00:00:00 to 2020-12-16 00:00:00\n",
      "Test set shape: (6444, 19), Time range: 2021-01-16 00:00:00 to 2023-12-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# main.ipynb content to be used in Jupyter Notebook format\n",
    "\n",
    "# Section 1: Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from src.adm_prednet.stacked_model import ADMStackedModel\n",
    "from src.adm_prednet.pe_utils import add_temporal_pe, add_spatial_pe\n",
    "from src.adm_prednet.masked_loss import masked_mse\n",
    "from src.adm_prednet.evaluate import evaluate_model\n",
    "from src.adm_prednet.save_predictions import save_predictions_to_csv\n",
    "from src.adm_prednet.visualize_utils import plot_comparison\n",
    "from src.adm_prednet.train import train_adm_model\n",
    "from src.data_utils import split_data_chronologically\n",
    "\n",
    "\n",
    "# Section 2: Load config\n",
    "import yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Section 3: Load data\n",
    "full_df = pd.read_csv(cfg['data']['csv_path'])\n",
    "train_df,val_df,test_df = split_data_chronologically(\n",
    "    full_df,\n",
    "    cfg\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61e8dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data: Train ends 2017-12-31 00:00:00, Validation ends 2020-12-31 00:00:00\n",
      "Warning: Time column 'time' is not datetime. Attempting conversion.\n",
      "Train set shape: (251316, 19), Time range: 1901-01-16 00:00:00 to 2017-12-16 00:00:00\n",
      "Validation set shape: (6444, 19), Time range: 2018-01-16 00:00:00 to 2020-12-16 00:00:00\n",
      "Test set shape: (6444, 19), Time range: 2021-01-16 00:00:00 to 2023-12-16 00:00:00\n",
      "--- Starting Data Gridding Process (Fixed Step Method) ---\n",
      "Using fixed grid step of: 0.5 degrees\n",
      "Grid boundaries: LAT (6.25, 20.25), LON (97.75, 105.25)\n",
      "Calculated grid dimensions: Height=29, Width=16\n",
      "Created 2D validity mask (29x16) with 179 valid data pixels.\n",
      "Pivoting data into a 4D tensor of shape (36, 29, 16, 15)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['row_idx'] = ((df[lat_col] - lat_min) / fixed_step).round().astype(int)\n",
      "c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\grid_utils.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['col_idx'] = ((df[lon_col] - lon_min) / fixed_step).round().astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Gridding Process Finished ---\n"
     ]
    }
   ],
   "source": [
    "# main.ipynb content to be used in Jupyter Notebook format\n",
    "\n",
    "# Section 1: Setup\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from src.adm_prednet.stacked_model import ADMStackedModel\n",
    "from src.adm_prednet.pe_utils import add_temporal_pe, add_spatial_pe\n",
    "from src.adm_prednet.masked_loss import masked_mse\n",
    "from src.adm_prednet.evaluate import evaluate_model\n",
    "from src.adm_prednet.save_predictions import save_predictions_to_csv\n",
    "from src.adm_prednet.visualize_utils import plot_comparison\n",
    "from src.grid_utils import create_gridded_data\n",
    "from src.data_utils import split_data_chronologically\n",
    "\n",
    "# Section 2: Load config\n",
    "import yaml\n",
    "with open(\"config.yaml\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Section 3: Load and Grid Data\n",
    "full_df = pd.read_csv(cfg['data']['csv_path'])\n",
    "train_df, val_df,test_df = split_data_chronologically(full_df, cfg)\n",
    "gridded_tensor, land_mask = create_gridded_data(val_df, cfg)\n",
    "\n",
    "# Section 4: Dataset Class from Grid\n",
    "from torch.utils.data import Dataset\n",
    "class GriddedSeq2SeqDataset(Dataset):\n",
    "    def __init__(self, gridded_tensor, input_steps=12, target_steps=1, target_indices=[0, 1]):\n",
    "        self.data = torch.tensor(gridded_tensor, dtype=torch.float32)\n",
    "        self.input_steps = input_steps\n",
    "        self.target_steps = target_steps\n",
    "        self.target_indices = target_indices\n",
    "        self.grid_shape = self.data.shape[1:3]  # (H, W)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.input_steps - self.target_steps + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.data[idx:idx + self.input_steps]  # [T, H, W, C]\n",
    "        X = X.permute(0, 3, 1, 2).float()           # â†’ [T, C, H, W]\n",
    "        Y = self.data[idx + self.input_steps]      # [H, W, C]\n",
    "        Y = Y[..., self.target_indices].permute(2, 0, 1)  # [C_target, H, W]\n",
    "        return X, Y\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eee1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GriddedSeq2SeqDataset(\n",
    "    gridded_tensor,\n",
    "    input_steps=cfg['training']['input_steps'],\n",
    "    target_steps=cfg['training']['output_steps'],\n",
    "    target_indices=list(range(len(cfg['model']['output_targets'])))\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg['training']['batch_size'], shuffle=True)\n",
    "\n",
    "val_dataset = GriddedSeq2SeqDataset(\n",
    "    gridded_tensor,\n",
    "    input_steps=cfg['training']['input_steps'],\n",
    "    target_steps=cfg['training']['output_steps'],\n",
    "    target_indices=list(range(len(cfg['model']['output_targets'])))\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg['training']['batch_size'], shuffle=False)\n",
    "test_dataset = GriddedSeq2SeqDataset(\n",
    "    gridded_tensor,\n",
    "    input_steps=cfg['training']['input_steps'],\n",
    "    target_steps=cfg['training']['output_steps'],\n",
    "    target_indices=list(range(len(cfg['model']['output_targets'])))\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg['training']['batch_size'], shuffle=False)\n",
    "# Section 4: Load model\n",
    "model = ADMStackedModel(\n",
    "    input_channels=cfg['model']['input_channels'],\n",
    "    hidden_channels=cfg['model']['hidden_channels'],\n",
    "    n_layers=cfg['model']['n_layers'],\n",
    "    output_targets=cfg['model']['output_targets']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27eaf377",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GriddedSeq2SeqDataset(\n",
    "    gridded_tensor,\n",
    "    input_steps=cfg['training']['input_steps'],\n",
    "    target_steps=cfg['training']['output_steps'],\n",
    "    target_indices=list(range(len(cfg['model']['output_targets'])))\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg['training']['batch_size'], shuffle=False)\n",
    "\n",
    "# Section 5: Load model\n",
    "model = ADMStackedModel(\n",
    "    input_channels=cfg['model']['input_channels'],\n",
    "    hidden_channels=cfg['model']['hidden_channels'],\n",
    "    n_layers=cfg['model']['n_layers'],\n",
    "    output_targets=cfg['model']['output_targets']\n",
    ").cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e67718df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.1 is exactly one major version older than the runtime version 6.31.1 at api.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "land_mask = torch.tensor(land_mask, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b28e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing purposes\n",
    "train_loader = DataLoader(val_dataset, batch_size=cfg['training']['batch_size'], shuffle=True)\n",
    "train_dataset = val_dataset\n",
    "test_dataset = val_dataset\n",
    "test_loader = val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11ba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.ipynb or objective function\n",
    "\n",
    "def objective(trial, config, train_dataset, val_dataset, land_mask):\n",
    "    \"\"\"\n",
    "    Objective function to be used by Optuna for hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    # Hyperparameters to be tuned\n",
    "    n_layers = trial.suggest_int('n_layers', 2, 5)\n",
    "    \n",
    "    # Define hidden_channels as a list of n_layers\n",
    "    hidden_channels = [trial.categorical(f'hidden_channels_{i}', [32,64,128,256]) for i in range(n_layers)]\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 2e-5, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.3)\n",
    "    batch_size = trial.suggest_int('batch_size', 8, 32, step=8)\n",
    "\n",
    "    # Ensure hidden_channels is a list and its length matches n_layers\n",
    "    assert isinstance(hidden_channels, list) and len(hidden_channels) == n_layers, \"hidden_channels must be a list with length matching n_layers\"\n",
    "    \n",
    "    # Create DataLoader with the current batch size\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Update the config with the tuned hyperparameters\n",
    "    config['model']['hidden_channels'] = hidden_channels\n",
    "    config['model']['n_layers'] = n_layers\n",
    "    config['training']['learning_rate'] = learning_rate\n",
    "    config['training']['dropout_rate'] = dropout_rate\n",
    "\n",
    "    # Train the model and get the evaluation metrics\n",
    "    model = train_adm_model(train_loader, config, val_loader=val_loader, land_mask=land_mask)\n",
    "\n",
    "    # Evaluate the model\n",
    "    metrics = evaluate_model(model, val_loader, land_mask, output_targets=config['model']['output_targets'])\n",
    "\n",
    "    # Return RMSE (or other metrics) for Optuna to optimize\n",
    "    return metrics['PET']['RMSE']  # You can choose other targets if you want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda630e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([29, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "land_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c9a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 18:13:32,071] A new study created in memory with name: no-name-8ec7f656-84e2-464c-b546-851c557fa579\n",
      "C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_27456\\2475644686.py:13: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_27456\\2475644686.py:14: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.2087\n",
      "Epoch 1 - Validation Loss: 0.2067\n",
      "Epoch 2 - Train Loss: 0.2068\n",
      "Epoch 2 - Validation Loss: 0.2044\n",
      "Epoch 3 - Train Loss: 0.2046\n",
      "Epoch 3 - Validation Loss: 0.2021\n",
      "Epoch 4 - Train Loss: 0.2017\n",
      "Epoch 4 - Validation Loss: 0.1994\n",
      "Epoch 5 - Train Loss: 0.1995\n",
      "Epoch 5 - Validation Loss: 0.1965\n",
      "Epoch 6 - Train Loss: 0.1967\n",
      "Epoch 6 - Validation Loss: 0.1932\n",
      "Epoch 7 - Train Loss: 0.1936\n",
      "Epoch 7 - Validation Loss: 0.1895\n",
      "Epoch 8 - Train Loss: 0.1897\n",
      "Epoch 8 - Validation Loss: 0.1859\n",
      "Epoch 9 - Train Loss: 0.1872\n",
      "Epoch 9 - Validation Loss: 0.1833\n",
      "Epoch 10 - Train Loss: 0.1831\n",
      "Epoch 10 - Validation Loss: 0.1835\n",
      "Epoch 11 - Train Loss: 0.1852\n",
      "Epoch 11 - Validation Loss: 0.1838\n",
      "Epoch 12 - Train Loss: 0.1835\n",
      "Epoch 12 - Validation Loss: 0.1812\n",
      "Epoch 13 - Train Loss: 0.1827\n",
      "Epoch 13 - Validation Loss: 0.1781\n",
      "Epoch 14 - Train Loss: 0.1789\n",
      "Epoch 14 - Validation Loss: 0.1756\n",
      "Epoch 15 - Train Loss: 0.1775\n",
      "Epoch 15 - Validation Loss: 0.1737\n",
      "Epoch 16 - Train Loss: 0.1759\n",
      "Epoch 16 - Validation Loss: 0.1720\n",
      "Epoch 17 - Train Loss: 0.1723\n",
      "Epoch 17 - Validation Loss: 0.1699\n",
      "Epoch 18 - Train Loss: 0.1715\n",
      "Epoch 18 - Validation Loss: 0.1671\n",
      "Epoch 19 - Train Loss: 0.1671\n",
      "Epoch 19 - Validation Loss: 0.1636\n",
      "Epoch 20 - Train Loss: 0.1642\n",
      "Epoch 20 - Validation Loss: 0.1593\n",
      "Epoch 21 - Train Loss: 0.1593\n",
      "Epoch 21 - Validation Loss: 0.1544\n",
      "Epoch 22 - Train Loss: 0.1571\n",
      "Epoch 22 - Validation Loss: 0.1493\n",
      "Epoch 23 - Train Loss: 0.1520\n",
      "Epoch 23 - Validation Loss: 0.1447\n",
      "Epoch 24 - Train Loss: 0.1462\n",
      "Epoch 24 - Validation Loss: 0.1404\n",
      "Epoch 25 - Train Loss: 0.1431\n",
      "Epoch 25 - Validation Loss: 0.1353\n",
      "Epoch 26 - Train Loss: 0.1406\n",
      "Epoch 26 - Validation Loss: 0.1286\n",
      "Epoch 27 - Train Loss: 0.1321\n",
      "Epoch 27 - Validation Loss: 0.1210\n",
      "Epoch 28 - Train Loss: 0.1252\n",
      "Epoch 28 - Validation Loss: 0.1135\n",
      "Epoch 29 - Train Loss: 0.1194\n",
      "Epoch 29 - Validation Loss: 0.1066\n",
      "Epoch 30 - Train Loss: 0.1084\n",
      "Epoch 30 - Validation Loss: 0.1002\n",
      "Epoch 31 - Train Loss: 0.1070\n",
      "Epoch 31 - Validation Loss: 0.0940\n",
      "Epoch 32 - Train Loss: 0.1036\n",
      "Epoch 32 - Validation Loss: 0.0898\n",
      "Epoch 33 - Train Loss: 0.0966\n",
      "Epoch 33 - Validation Loss: 0.0878\n",
      "Epoch 34 - Train Loss: 0.0932\n",
      "Epoch 34 - Validation Loss: 0.0860\n",
      "Epoch 35 - Train Loss: 0.0883\n",
      "Epoch 35 - Validation Loss: 0.0829\n",
      "Epoch 36 - Train Loss: 0.0885\n",
      "Epoch 36 - Validation Loss: 0.0793\n",
      "Epoch 37 - Train Loss: 0.0869\n",
      "Epoch 37 - Validation Loss: 0.0754\n",
      "Epoch 38 - Train Loss: 0.0824\n",
      "Epoch 38 - Validation Loss: 0.0718\n",
      "Epoch 39 - Train Loss: 0.0732\n",
      "Epoch 39 - Validation Loss: 0.0683\n",
      "Epoch 40 - Train Loss: 0.0791\n",
      "Epoch 40 - Validation Loss: 0.0649\n",
      "Epoch 41 - Train Loss: 0.0812\n",
      "Epoch 41 - Validation Loss: 0.0626\n",
      "Epoch 42 - Train Loss: 0.0736\n",
      "Epoch 42 - Validation Loss: 0.0621\n",
      "Epoch 43 - Train Loss: 0.0717\n",
      "Epoch 43 - Validation Loss: 0.0599\n",
      "Epoch 44 - Train Loss: 0.0680\n",
      "Epoch 44 - Validation Loss: 0.0557\n",
      "Epoch 45 - Train Loss: 0.0676\n",
      "Epoch 45 - Validation Loss: 0.0522\n",
      "Epoch 46 - Train Loss: 0.0584\n",
      "Epoch 46 - Validation Loss: 0.0510\n",
      "Epoch 47 - Train Loss: 0.0676\n",
      "Epoch 47 - Validation Loss: 0.0494\n",
      "Epoch 48 - Train Loss: 0.0578\n",
      "Epoch 48 - Validation Loss: 0.0487\n",
      "Epoch 49 - Train Loss: 0.0574\n",
      "Epoch 49 - Validation Loss: 0.0484\n",
      "Epoch 50 - Train Loss: 0.0516\n",
      "Epoch 50 - Validation Loss: 0.0490\n",
      "Epoch 51 - Train Loss: 0.0575\n",
      "Epoch 51 - Validation Loss: 0.0489\n",
      "Epoch 52 - Train Loss: 0.0528\n",
      "Epoch 52 - Validation Loss: 0.0472\n",
      "Epoch 53 - Train Loss: 0.0536\n",
      "Epoch 53 - Validation Loss: 0.0447\n",
      "Epoch 54 - Train Loss: 0.0493\n",
      "Epoch 54 - Validation Loss: 0.0424\n",
      "Epoch 55 - Train Loss: 0.0469\n",
      "Epoch 55 - Validation Loss: 0.0412\n",
      "Epoch 56 - Train Loss: 0.0475\n",
      "Epoch 56 - Validation Loss: 0.0407\n",
      "Epoch 57 - Train Loss: 0.0466\n",
      "Epoch 57 - Validation Loss: 0.0400\n",
      "Epoch 58 - Train Loss: 0.0491\n",
      "Epoch 58 - Validation Loss: 0.0390\n",
      "Epoch 59 - Train Loss: 0.0450\n",
      "Epoch 59 - Validation Loss: 0.0379\n",
      "Epoch 60 - Train Loss: 0.0466\n",
      "Epoch 60 - Validation Loss: 0.0376\n",
      "Epoch 61 - Train Loss: 0.0480\n",
      "Epoch 61 - Validation Loss: 0.0380\n",
      "Epoch 62 - Train Loss: 0.0414\n",
      "Epoch 62 - Validation Loss: 0.0383\n",
      "Epoch 63 - Train Loss: 0.0463\n",
      "Epoch 63 - Validation Loss: 0.0379\n",
      "Epoch 64 - Train Loss: 0.0390\n",
      "Epoch 64 - Validation Loss: 0.0366\n",
      "Epoch 65 - Train Loss: 0.0455\n",
      "Epoch 65 - Validation Loss: 0.0350\n",
      "Epoch 66 - Train Loss: 0.0396\n",
      "Epoch 66 - Validation Loss: 0.0346\n",
      "Epoch 67 - Train Loss: 0.0420\n",
      "Epoch 67 - Validation Loss: 0.0352\n",
      "Epoch 68 - Train Loss: 0.0418\n",
      "Epoch 68 - Validation Loss: 0.0355\n",
      "Epoch 69 - Train Loss: 0.0420\n",
      "Epoch 69 - Validation Loss: 0.0346\n",
      "Epoch 70 - Train Loss: 0.0392\n",
      "Epoch 70 - Validation Loss: 0.0337\n",
      "Epoch 71 - Train Loss: 0.0387\n",
      "Epoch 71 - Validation Loss: 0.0330\n",
      "Epoch 72 - Train Loss: 0.0369\n",
      "Epoch 72 - Validation Loss: 0.0328\n",
      "Epoch 73 - Train Loss: 0.0367\n",
      "Epoch 73 - Validation Loss: 0.0329\n",
      "Epoch 74 - Train Loss: 0.0403\n",
      "Epoch 74 - Validation Loss: 0.0330\n",
      "Epoch 75 - Train Loss: 0.0398\n",
      "Epoch 75 - Validation Loss: 0.0330\n",
      "Epoch 76 - Train Loss: 0.0418\n",
      "Epoch 76 - Validation Loss: 0.0321\n",
      "Epoch 77 - Train Loss: 0.0421\n",
      "Epoch 77 - Validation Loss: 0.0310\n",
      "Epoch 78 - Train Loss: 0.0342\n",
      "Epoch 78 - Validation Loss: 0.0302\n",
      "Epoch 79 - Train Loss: 0.0361\n",
      "Epoch 79 - Validation Loss: 0.0300\n",
      "Epoch 80 - Train Loss: 0.0399\n",
      "Epoch 80 - Validation Loss: 0.0302\n",
      "Epoch 81 - Train Loss: 0.0380\n",
      "Epoch 81 - Validation Loss: 0.0306\n",
      "Epoch 82 - Train Loss: 0.0362\n",
      "Epoch 82 - Validation Loss: 0.0302\n",
      "Epoch 83 - Train Loss: 0.0418\n",
      "Epoch 83 - Validation Loss: 0.0294\n",
      "Epoch 84 - Train Loss: 0.0357\n",
      "Epoch 84 - Validation Loss: 0.0286\n",
      "Epoch 85 - Train Loss: 0.0361\n",
      "Epoch 85 - Validation Loss: 0.0284\n",
      "Epoch 86 - Train Loss: 0.0290\n",
      "Epoch 86 - Validation Loss: 0.0284\n",
      "Epoch 87 - Train Loss: 0.0322\n",
      "Epoch 87 - Validation Loss: 0.0280\n",
      "Epoch 88 - Train Loss: 0.0337\n",
      "Epoch 88 - Validation Loss: 0.0276\n",
      "Epoch 89 - Train Loss: 0.0354\n",
      "Epoch 89 - Validation Loss: 0.0272\n",
      "Epoch 90 - Train Loss: 0.0309\n",
      "Epoch 90 - Validation Loss: 0.0271\n",
      "Epoch 91 - Train Loss: 0.0321\n",
      "Epoch 91 - Validation Loss: 0.0275\n",
      "Epoch 92 - Train Loss: 0.0323\n",
      "Epoch 92 - Validation Loss: 0.0272\n",
      "Epoch 93 - Train Loss: 0.0316\n",
      "Epoch 93 - Validation Loss: 0.0263\n",
      "Epoch 94 - Train Loss: 0.0305\n",
      "Epoch 94 - Validation Loss: 0.0256\n",
      "Epoch 95 - Train Loss: 0.0345\n",
      "Epoch 95 - Validation Loss: 0.0253\n",
      "Epoch 96 - Train Loss: 0.0334\n",
      "Epoch 96 - Validation Loss: 0.0253\n",
      "Epoch 97 - Train Loss: 0.0352\n",
      "Epoch 97 - Validation Loss: 0.0249\n",
      "Epoch 98 - Train Loss: 0.0314\n",
      "Epoch 98 - Validation Loss: 0.0243\n",
      "Epoch 99 - Train Loss: 0.0304\n",
      "Epoch 99 - Validation Loss: 0.0241\n",
      "Epoch 100 - Train Loss: 0.0343\n",
      "Epoch 100 - Validation Loss: 0.0246\n",
      "Epoch 101 - Train Loss: 0.0337\n",
      "Epoch 101 - Validation Loss: 0.0253\n",
      "Epoch 102 - Train Loss: 0.0300\n",
      "Epoch 102 - Validation Loss: 0.0248\n",
      "Epoch 103 - Train Loss: 0.0321\n",
      "Epoch 103 - Validation Loss: 0.0235\n",
      "Epoch 104 - Train Loss: 0.0289\n",
      "Epoch 104 - Validation Loss: 0.0233\n",
      "Epoch 105 - Train Loss: 0.0308\n",
      "Epoch 105 - Validation Loss: 0.0232\n",
      "Epoch 106 - Train Loss: 0.0336\n",
      "Epoch 106 - Validation Loss: 0.0228\n",
      "Epoch 107 - Train Loss: 0.0286\n",
      "Epoch 107 - Validation Loss: 0.0226\n",
      "Epoch 108 - Train Loss: 0.0284\n",
      "Epoch 108 - Validation Loss: 0.0222\n",
      "Epoch 109 - Train Loss: 0.0291\n",
      "Epoch 109 - Validation Loss: 0.0226\n",
      "Epoch 110 - Train Loss: 0.0273\n",
      "Epoch 110 - Validation Loss: 0.0236\n",
      "Epoch 111 - Train Loss: 0.0365\n",
      "Epoch 111 - Validation Loss: 0.0227\n",
      "Epoch 112 - Train Loss: 0.0283\n",
      "Epoch 112 - Validation Loss: 0.0220\n",
      "Epoch 113 - Train Loss: 0.0289\n",
      "Epoch 113 - Validation Loss: 0.0227\n",
      "Epoch 114 - Train Loss: 0.0292\n",
      "Epoch 114 - Validation Loss: 0.0235\n",
      "Epoch 115 - Train Loss: 0.0300\n",
      "Epoch 115 - Validation Loss: 0.0227\n",
      "Epoch 116 - Train Loss: 0.0303\n",
      "Epoch 116 - Validation Loss: 0.0211\n",
      "Epoch 117 - Train Loss: 0.0244\n",
      "Epoch 117 - Validation Loss: 0.0207\n",
      "Epoch 118 - Train Loss: 0.0231\n",
      "Epoch 118 - Validation Loss: 0.0215\n",
      "Epoch 119 - Train Loss: 0.0316\n",
      "Epoch 119 - Validation Loss: 0.0217\n",
      "Epoch 120 - Train Loss: 0.0242\n",
      "Epoch 120 - Validation Loss: 0.0215\n",
      "Epoch 121 - Train Loss: 0.0267\n",
      "Epoch 121 - Validation Loss: 0.0203\n",
      "Epoch 122 - Train Loss: 0.0257\n",
      "Epoch 122 - Validation Loss: 0.0201\n",
      "Epoch 123 - Train Loss: 0.0255\n",
      "Epoch 123 - Validation Loss: 0.0211\n",
      "Epoch 124 - Train Loss: 0.0299\n",
      "Epoch 124 - Validation Loss: 0.0220\n",
      "Epoch 125 - Train Loss: 0.0288\n",
      "Epoch 125 - Validation Loss: 0.0216\n",
      "Epoch 126 - Train Loss: 0.0289\n",
      "Epoch 126 - Validation Loss: 0.0205\n",
      "Epoch 127 - Train Loss: 0.0295\n",
      "Epoch 127 - Validation Loss: 0.0204\n",
      "Epoch 128 - Train Loss: 0.0288\n",
      "Epoch 128 - Validation Loss: 0.0203\n",
      "Epoch 129 - Train Loss: 0.0239\n",
      "Epoch 129 - Validation Loss: 0.0201\n",
      "Epoch 130 - Train Loss: 0.0239\n",
      "Epoch 130 - Validation Loss: 0.0199\n",
      "Epoch 131 - Train Loss: 0.0234\n",
      "Epoch 131 - Validation Loss: 0.0198\n",
      "Epoch 132 - Train Loss: 0.0234\n",
      "Epoch 132 - Validation Loss: 0.0196\n",
      "Epoch 133 - Train Loss: 0.0238\n",
      "Epoch 133 - Validation Loss: 0.0196\n",
      "Epoch 134 - Train Loss: 0.0270\n",
      "Epoch 134 - Validation Loss: 0.0195\n",
      "Epoch 135 - Train Loss: 0.0270\n",
      "Epoch 135 - Validation Loss: 0.0195\n",
      "Epoch 136 - Train Loss: 0.0269\n",
      "Epoch 136 - Validation Loss: 0.0195\n",
      "Epoch 137 - Train Loss: 0.0226\n",
      "Epoch 137 - Validation Loss: 0.0195\n",
      "Epoch 138 - Train Loss: 0.0228\n",
      "Epoch 138 - Validation Loss: 0.0196\n",
      "Epoch 139 - Train Loss: 0.0241\n",
      "Epoch 139 - Validation Loss: 0.0196\n",
      "Epoch 140 - Train Loss: 0.0231\n",
      "Epoch 140 - Validation Loss: 0.0196\n",
      "Epoch 141 - Train Loss: 0.0229\n",
      "Epoch 141 - Validation Loss: 0.0196\n",
      "Epoch 142 - Train Loss: 0.0270\n",
      "Epoch 142 - Validation Loss: 0.0196\n",
      "Epoch 143 - Train Loss: 0.0266\n",
      "Epoch 143 - Validation Loss: 0.0196\n",
      "Epoch 144 - Train Loss: 0.0244\n",
      "Epoch 144 - Validation Loss: 0.0196\n",
      "Epoch 145 - Train Loss: 0.0233\n",
      "Epoch 145 - Validation Loss: 0.0196\n",
      "Epoch 146 - Train Loss: 0.0256\n",
      "Epoch 146 - Validation Loss: 0.0196\n",
      "Epoch 147 - Train Loss: 0.0288\n",
      "Epoch 147 - Validation Loss: 0.0196\n",
      "Epoch 148 - Train Loss: 0.0229\n",
      "Epoch 148 - Validation Loss: 0.0196\n",
      "Epoch 149 - Train Loss: 0.0245\n",
      "Epoch 149 - Validation Loss: 0.0196\n",
      "Epoch 150 - Train Loss: 0.0247\n",
      "Epoch 150 - Validation Loss: 0.0196\n",
      "Epoch 151 - Train Loss: 0.0254\n",
      "Epoch 151 - Validation Loss: 0.0196\n",
      "Epoch 152 - Train Loss: 0.0295\n",
      "Epoch 152 - Validation Loss: 0.0196\n",
      "Epoch 153 - Train Loss: 0.0248\n",
      "Epoch 153 - Validation Loss: 0.0196\n",
      "Epoch 154 - Train Loss: 0.0267\n",
      "Epoch 154 - Validation Loss: 0.0196\n",
      "Epoch 155 - Train Loss: 0.0244\n",
      "Epoch 155 - Validation Loss: 0.0196\n",
      "Early stopping triggered at epoch 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 18:15:07,228] Trial 0 finished with value: 0.24159036576747894 and parameters: {'n_layers': 3, 'hidden_channels_0': 20, 'hidden_channels_1': 97, 'hidden_channels_2': 106, 'learning_rate': 0.00016608215625672567, 'dropout_rate': 0.24151873138803226, 'batch_size': 24}. Best is trial 0 with value: 0.24159036576747894.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found by Optuna: {'n_layers': 3, 'hidden_channels_0': 20, 'hidden_channels_1': 97, 'hidden_channels_2': 106, 'learning_rate': 0.00016608215625672567, 'dropout_rate': 0.24151873138803226, 'batch_size': 24}\n",
      "Reconstructed hidden_channels: [20, 97, 106]\n",
      "Epoch 1 - Train Loss: 0.2134\n",
      "Epoch 1 - Validation Loss: 0.2079\n",
      "Epoch 2 - Train Loss: 0.2067\n",
      "Epoch 2 - Validation Loss: 0.2015\n",
      "Epoch 3 - Train Loss: 0.1997\n",
      "Epoch 3 - Validation Loss: 0.1947\n",
      "Epoch 4 - Train Loss: 0.1933\n",
      "Epoch 4 - Validation Loss: 0.1878\n",
      "Epoch 5 - Train Loss: 0.1848\n",
      "Epoch 5 - Validation Loss: 0.1850\n",
      "Epoch 6 - Train Loss: 0.1899\n",
      "Epoch 6 - Validation Loss: 0.1865\n",
      "Epoch 7 - Train Loss: 0.1869\n",
      "Epoch 7 - Validation Loss: 0.1809\n",
      "Epoch 8 - Train Loss: 0.1812\n",
      "Epoch 8 - Validation Loss: 0.1775\n",
      "Epoch 9 - Train Loss: 0.1774\n",
      "Epoch 9 - Validation Loss: 0.1748\n",
      "Epoch 10 - Train Loss: 0.1749\n",
      "Epoch 10 - Validation Loss: 0.1711\n",
      "Epoch 11 - Train Loss: 0.1699\n",
      "Epoch 11 - Validation Loss: 0.1651\n",
      "Epoch 12 - Train Loss: 0.1638\n",
      "Epoch 12 - Validation Loss: 0.1565\n",
      "Epoch 13 - Train Loss: 0.1544\n",
      "Epoch 13 - Validation Loss: 0.1463\n",
      "Epoch 14 - Train Loss: 0.1492\n",
      "Epoch 14 - Validation Loss: 0.1359\n",
      "Epoch 15 - Train Loss: 0.1343\n",
      "Epoch 15 - Validation Loss: 0.1249\n",
      "Epoch 16 - Train Loss: 0.1293\n",
      "Epoch 16 - Validation Loss: 0.1109\n",
      "Epoch 17 - Train Loss: 0.1116\n",
      "Epoch 17 - Validation Loss: 0.0992\n",
      "Epoch 18 - Train Loss: 0.1004\n",
      "Epoch 18 - Validation Loss: 0.0904\n",
      "Epoch 19 - Train Loss: 0.0923\n",
      "Epoch 19 - Validation Loss: 0.0841\n",
      "Epoch 20 - Train Loss: 0.0885\n",
      "Epoch 20 - Validation Loss: 0.0802\n",
      "Epoch 21 - Train Loss: 0.0926\n",
      "Epoch 21 - Validation Loss: 0.0748\n",
      "Epoch 22 - Train Loss: 0.0794\n",
      "Epoch 22 - Validation Loss: 0.0692\n",
      "Epoch 23 - Train Loss: 0.0749\n",
      "Epoch 23 - Validation Loss: 0.0636\n",
      "Epoch 24 - Train Loss: 0.0676\n",
      "Epoch 24 - Validation Loss: 0.0571\n",
      "Epoch 25 - Train Loss: 0.0624\n",
      "Epoch 25 - Validation Loss: 0.0551\n",
      "Epoch 26 - Train Loss: 0.0634\n",
      "Epoch 26 - Validation Loss: 0.0484\n",
      "Epoch 27 - Train Loss: 0.0528\n",
      "Epoch 27 - Validation Loss: 0.0474\n",
      "Epoch 28 - Train Loss: 0.0555\n",
      "Epoch 28 - Validation Loss: 0.0451\n",
      "Epoch 29 - Train Loss: 0.0512\n",
      "Epoch 29 - Validation Loss: 0.0437\n",
      "Epoch 30 - Train Loss: 0.0514\n",
      "Epoch 30 - Validation Loss: 0.0404\n",
      "Epoch 31 - Train Loss: 0.0479\n",
      "Epoch 31 - Validation Loss: 0.0386\n",
      "Epoch 32 - Train Loss: 0.0425\n",
      "Epoch 32 - Validation Loss: 0.0370\n",
      "Epoch 33 - Train Loss: 0.0424\n",
      "Epoch 33 - Validation Loss: 0.0359\n",
      "Epoch 34 - Train Loss: 0.0428\n",
      "Epoch 34 - Validation Loss: 0.0352\n",
      "Epoch 35 - Train Loss: 0.0417\n",
      "Epoch 35 - Validation Loss: 0.0345\n",
      "Epoch 36 - Train Loss: 0.0376\n",
      "Epoch 36 - Validation Loss: 0.0342\n",
      "Epoch 37 - Train Loss: 0.0416\n",
      "Epoch 37 - Validation Loss: 0.0323\n",
      "Epoch 38 - Train Loss: 0.0361\n",
      "Epoch 38 - Validation Loss: 0.0315\n",
      "Epoch 39 - Train Loss: 0.0398\n",
      "Epoch 39 - Validation Loss: 0.0314\n",
      "Epoch 40 - Train Loss: 0.0396\n",
      "Epoch 40 - Validation Loss: 0.0306\n",
      "Epoch 41 - Train Loss: 0.0383\n",
      "Epoch 41 - Validation Loss: 0.0295\n",
      "Epoch 42 - Train Loss: 0.0363\n",
      "Epoch 42 - Validation Loss: 0.0286\n",
      "Epoch 43 - Train Loss: 0.0361\n",
      "Epoch 43 - Validation Loss: 0.0285\n",
      "Epoch 44 - Train Loss: 0.0350\n",
      "Epoch 44 - Validation Loss: 0.0284\n",
      "Epoch 45 - Train Loss: 0.0327\n",
      "Epoch 45 - Validation Loss: 0.0277\n",
      "Epoch 46 - Train Loss: 0.0365\n",
      "Epoch 46 - Validation Loss: 0.0270\n",
      "Epoch 47 - Train Loss: 0.0348\n",
      "Epoch 47 - Validation Loss: 0.0257\n",
      "Epoch 48 - Train Loss: 0.0342\n",
      "Epoch 48 - Validation Loss: 0.0257\n",
      "Epoch 49 - Train Loss: 0.0317\n",
      "Epoch 49 - Validation Loss: 0.0251\n",
      "Epoch 50 - Train Loss: 0.0336\n",
      "Epoch 50 - Validation Loss: 0.0246\n",
      "Epoch 51 - Train Loss: 0.0291\n",
      "Epoch 51 - Validation Loss: 0.0253\n",
      "Epoch 52 - Train Loss: 0.0312\n",
      "Epoch 52 - Validation Loss: 0.0235\n",
      "Epoch 53 - Train Loss: 0.0294\n",
      "Epoch 53 - Validation Loss: 0.0241\n",
      "Epoch 54 - Train Loss: 0.0284\n",
      "Epoch 54 - Validation Loss: 0.0232\n",
      "Epoch 55 - Train Loss: 0.0275\n",
      "Epoch 55 - Validation Loss: 0.0236\n",
      "Epoch 56 - Train Loss: 0.0321\n",
      "Epoch 56 - Validation Loss: 0.0235\n",
      "Epoch 57 - Train Loss: 0.0283\n",
      "Epoch 57 - Validation Loss: 0.0223\n",
      "Epoch 58 - Train Loss: 0.0290\n",
      "Epoch 58 - Validation Loss: 0.0229\n",
      "Epoch 59 - Train Loss: 0.0276\n",
      "Epoch 59 - Validation Loss: 0.0215\n",
      "Epoch 60 - Train Loss: 0.0268\n",
      "Epoch 60 - Validation Loss: 0.0224\n",
      "Epoch 61 - Train Loss: 0.0289\n",
      "Epoch 61 - Validation Loss: 0.0217\n",
      "Epoch 62 - Train Loss: 0.0245\n",
      "Epoch 62 - Validation Loss: 0.0212\n",
      "Epoch 63 - Train Loss: 0.0265\n",
      "Epoch 63 - Validation Loss: 0.0214\n",
      "Epoch 64 - Train Loss: 0.0292\n",
      "Epoch 64 - Validation Loss: 0.0213\n",
      "Epoch 65 - Train Loss: 0.0281\n",
      "Epoch 65 - Validation Loss: 0.0211\n",
      "Epoch 66 - Train Loss: 0.0248\n",
      "Epoch 66 - Validation Loss: 0.0202\n",
      "Epoch 67 - Train Loss: 0.0263\n",
      "Epoch 67 - Validation Loss: 0.0199\n",
      "Epoch 68 - Train Loss: 0.0242\n",
      "Epoch 68 - Validation Loss: 0.0200\n",
      "Epoch 69 - Train Loss: 0.0249\n",
      "Epoch 69 - Validation Loss: 0.0205\n",
      "Epoch 70 - Train Loss: 0.0252\n",
      "Epoch 70 - Validation Loss: 0.0225\n",
      "Epoch 71 - Train Loss: 0.0256\n",
      "Epoch 71 - Validation Loss: 0.0201\n",
      "Epoch 72 - Train Loss: 0.0237\n",
      "Epoch 72 - Validation Loss: 0.0197\n",
      "Epoch 73 - Train Loss: 0.0269\n",
      "Epoch 73 - Validation Loss: 0.0193\n",
      "Epoch 74 - Train Loss: 0.0258\n",
      "Epoch 74 - Validation Loss: 0.0190\n",
      "Epoch 75 - Train Loss: 0.0259\n",
      "Epoch 75 - Validation Loss: 0.0189\n",
      "Epoch 76 - Train Loss: 0.0229\n",
      "Epoch 76 - Validation Loss: 0.0189\n",
      "Epoch 77 - Train Loss: 0.0235\n",
      "Epoch 77 - Validation Loss: 0.0188\n",
      "Epoch 78 - Train Loss: 0.0240\n",
      "Epoch 78 - Validation Loss: 0.0188\n",
      "Epoch 79 - Train Loss: 0.0247\n",
      "Epoch 79 - Validation Loss: 0.0188\n",
      "Epoch 80 - Train Loss: 0.0236\n",
      "Epoch 80 - Validation Loss: 0.0188\n",
      "Epoch 81 - Train Loss: 0.0242\n",
      "Epoch 81 - Validation Loss: 0.0188\n",
      "Epoch 82 - Train Loss: 0.0252\n",
      "Epoch 82 - Validation Loss: 0.0187\n",
      "Epoch 83 - Train Loss: 0.0239\n",
      "Epoch 83 - Validation Loss: 0.0186\n",
      "Epoch 84 - Train Loss: 0.0224\n",
      "Epoch 84 - Validation Loss: 0.0186\n",
      "Epoch 85 - Train Loss: 0.0216\n",
      "Epoch 85 - Validation Loss: 0.0185\n",
      "Epoch 86 - Train Loss: 0.0219\n",
      "Epoch 86 - Validation Loss: 0.0185\n",
      "Epoch 87 - Train Loss: 0.0240\n",
      "Epoch 87 - Validation Loss: 0.0184\n",
      "Epoch 88 - Train Loss: 0.0235\n",
      "Epoch 88 - Validation Loss: 0.0184\n",
      "Epoch 89 - Train Loss: 0.0219\n",
      "Epoch 89 - Validation Loss: 0.0185\n",
      "Epoch 90 - Train Loss: 0.0246\n",
      "Epoch 90 - Validation Loss: 0.0185\n",
      "Epoch 91 - Train Loss: 0.0244\n",
      "Epoch 91 - Validation Loss: 0.0185\n",
      "Epoch 92 - Train Loss: 0.0266\n",
      "Epoch 92 - Validation Loss: 0.0184\n",
      "Epoch 93 - Train Loss: 0.0241\n",
      "Epoch 93 - Validation Loss: 0.0184\n",
      "Epoch 94 - Train Loss: 0.0231\n",
      "Epoch 94 - Validation Loss: 0.0184\n",
      "Epoch 95 - Train Loss: 0.0230\n",
      "Epoch 95 - Validation Loss: 0.0184\n",
      "Epoch 96 - Train Loss: 0.0233\n",
      "Epoch 96 - Validation Loss: 0.0184\n",
      "Epoch 97 - Train Loss: 0.0256\n",
      "Epoch 97 - Validation Loss: 0.0184\n",
      "Epoch 98 - Train Loss: 0.0238\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m train_val_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_val_dataset, batch_size\u001b[38;5;241m=\u001b[39mcfg_final[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train the final model using the best configuration from Optuna\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_adm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_val_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mland_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mland_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Save the final model after training\u001b[39;00m\n\u001b[0;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(final_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\adm_prednet\\train.py:87\u001b[0m, in \u001b[0;36mtrain_adm_model\u001b[1;34m(dataset, config, val_loader, land_mask, log_path)\u001b[0m\n\u001b[0;32m     84\u001b[0m x_seq \u001b[38;5;241m=\u001b[39m add_spatial_pe(x_seq)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Forward pass to get predictions\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Compute validation loss for all targets\u001b[39;00m\n\u001b[0;32m     90\u001b[0m output_targets \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_targets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\adm_prednet\\stacked_model.py:48\u001b[0m, in \u001b[0;36mADMStackedModel.forward\u001b[1;34m(self, x_seq)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m             x_t \u001b[38;5;241m=\u001b[39m x_t \u001b[38;5;241m+\u001b[39m h[l\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Adding the previous layer's output as a residual\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m         x_t, c_s[l], c_t[l] \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_s\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_t\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get new hidden state\u001b[39;00m\n\u001b[0;32m     49\u001b[0m         h[l] \u001b[38;5;241m=\u001b[39m x_t  \u001b[38;5;66;03m# Update hidden state for the current layer\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Output head to produce final predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\src\\adm_prednet\\cell.py:74\u001b[0m, in \u001b[0;36mADMConvLSTMCell.forward\u001b[1;34m(self, x_t, h_prev, c_s_prev, c_t_prev)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Fusion gate\u001b[39;00m\n\u001b[0;32m     73\u001b[0m alpha_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([c_s, c_t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha_gate\u001b[49m\u001b[43m(\u001b[49m\u001b[43malpha_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m c \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m c_s \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m c_t\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Final hidden\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set up the Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, cfg, val_dataset, val_dataset, land_mask), n_trials=1)\n",
    "# After the Optuna study is finished\n",
    "\n",
    "# After the Optuna study is finished\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters found by Optuna:\", best_params)\n",
    "\n",
    "# Reconstruct the hidden_channels list from Optuna's best parameters\n",
    "n_layers = best_params['n_layers']  # Get the number of layers from the best trial\n",
    "hidden_channels = [best_params[f'hidden_channels_{i}'] for i in range(n_layers)]  # Reconstruct hidden_channels list\n",
    "\n",
    "print(\"Reconstructed hidden_channels:\", hidden_channels)\n",
    "\n",
    "# Assuming `cfg` is your model configuration\n",
    "cfg_final = cfg.copy()\n",
    "\n",
    "# Update the configuration dictionary with the best parameters from Optuna\n",
    "cfg_final['model']['hidden_channels'] = hidden_channels\n",
    "cfg_final['model']['n_layers'] = best_params['n_layers']\n",
    "cfg_final['training']['learning_rate'] = best_params['learning_rate']\n",
    "cfg_final['training']['dropout_rate'] = best_params['dropout_rate']\n",
    "cfg_final['training']['batch_size'] = best_params['batch_size']\n",
    "\n",
    "train_val_dataset = torch.utils.data.ConcatDataset([train_dataset, val_dataset])\n",
    "train_val_loader = DataLoader(train_val_dataset, batch_size=cfg_final['training']['batch_size'], shuffle=True)\n",
    "# Train the final model using the best configuration from Optuna\n",
    "final_model = train_adm_model(train_val_loader, cfg_final, val_loader=test_loader, land_mask=land_mask)\n",
    "\n",
    "# Save the final model after training\n",
    "torch.save(final_model.state_dict(), \"final_model.pth\")\n",
    "# 5. Save the final model\n",
    "torch.save(final_model.state_dict(), \"final_model.pth\")\n",
    "\n",
    "# 6. Evaluate the final model\n",
    "metrics = evaluate_model(final_model, test_loader, land_mask, output_targets=cfg['model']['output_targets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      " {'PET': {'RMSE': np.float32(0.19897145), 'MAE': np.float32(0.1597113), 'R2': 0.9328159093856812}, 'PRE': {'RMSE': np.float32(0.16868888), 'MAE': np.float32(0.13468586), 'R2': 0.9237540364265442}}\n",
      "Saved PET predictions to: run_outputs/predictions/PET_predictions.csv\n",
      "Saved PRE predictions to: run_outputs/predictions/PRE_predictions.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "metrics = evaluate_model(final_model, val_loader, land_mask, cfg['model']['output_targets'])\n",
    "print(\"Evaluation Metrics:\\n\", metrics)\n",
    "\n",
    "# Section 7: Predict and save\n",
    "save_predictions_to_csv(final_model, val_loader, land_mask, cfg['model']['output_targets'], val_df, cfg['output']['predictions_csv'])\n",
    "\n",
    "# Section 8: Visualize\n",
    "if cfg['output']['visualize_samples']:\n",
    "    for x_seq, y_true in val_loader:\n",
    "        x_seq = add_temporal_pe(x_seq)\n",
    "        x_seq = add_spatial_pe(x_seq)\n",
    "        y_pred = model(x_seq.cuda()).cpu()\n",
    "        for i in range(min(2, y_pred.shape[0])):\n",
    "            for c, target in enumerate(cfg['model']['output_targets']):\n",
    "                plot_comparison(y_pred[i, c], y_true[i, c], mask=land_mask, title=target)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc39e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation Metrics: {'PET': {'RMSE': np.float32(0.19897145), 'MAE': np.float32(0.1597113), 'R2': 0.9328159093856812}, 'PRE': {'RMSE': np.float32(0.16868888), 'MAE': np.float32(0.13468586), 'R2': 0.9237540364265442}}\n",
      "Saved PET predictions to: run_outputs/predictions/PET_predictions.csv\n",
      "Saved PRE predictions to: run_outputs/predictions/PRE_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Final Evaluation Metrics:\", metrics)\n",
    "# 7. Save predictions to CSV\n",
    "save_predictions_to_csv(\n",
    "    final_model,\n",
    "    val_loader,\n",
    "    land_mask,\n",
    "    output_targets=cfg['model']['output_targets'],\n",
    "    raw_df=val_df,\n",
    "    output_dir=cfg['output']['predictions_csv'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6faeca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
