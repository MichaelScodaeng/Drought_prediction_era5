# config_ConvLSTM_EncoderDecoder.yaml
# Configuration file for the Encoder-Decoder ConvLSTM model

project_setup:
  experiment_name: "PET_ConvLSTM_EncoderDecoder_Run"
  target_variable: "pet"
  random_seed: 42

data:
  raw_data_path:  "data/processed/full_scaled.csv"
  scaler_path: "data/processed/robust_scaler_pet.joblib"
  time_column: "time"
  lat_column: "lat"
  lon_column: "lon"
  features_to_grid: [
      'tmp', 'dtr', 'cld', 'tmx', 
      'tmn', 'wet', 'vap', 'soi', 
      'dmi', 'pdo', 'nino4', 
      'nino34', 'nino3', 'pre', 'pet'
    ]
  train_end_date: "2017-12-31"
  validation_end_date: "2020-12-31"

gridding:
  fixed_step: 0.5 

sequence_params:
  # n_steps_in is pre_seq_length for the new model
  n_steps_in: 12      
  # n_steps_out is aft_seq_length, how many steps to predict
  n_steps_out: 1 # You can experiment with predicting more steps, e.g., 3 or 6

convlstm_params:
  batch_size: 8
  
  trainer:
    max_epochs: 500
    patience_for_early_stopping: 50
    gradient_clip_val: 1.0
    enable_progress_bar: True
  
  tuning:
    n_trials: 20
    max_epochs: 1
    learning_rate: {low: 1.0e-5, high: 1.0e-4, log: True}
    hidden_dim_size: {choices: [128,256,512]}
    n_layers: {low: 2, high: 4}
    kernel_size: {choices: [3, 5]}
    dropout: {low: 0.0, high: 0.3}
    weight_decay: {low: 1.0e-6, high: 1.0e-3, log: True}
    batch_norm: [true, false]
    teacher_forcing_ratio: {low: 0.3, high: 0.8}

paths:
  models_base_dir: "models_saved" 

results:
  output_base_dir: "run_outputs" 
  metrics_filename: "global_convlstm_improved_metrics.json"
  predictions_filename: "global_convlstm_improved_full_predictions.csv"
  config_filename: "config_used_for_run.yaml"

