# config_LSTM_PyTorchLightning.yaml
# Configuration file for the LSTMPyTorchLightningLocalPipeline

project_setup:
  experiment_name: "PET_LSTM_Local_Run"  # Base name for the experiment; "_lstm_local" will be appended by the pipeline script for the output folder.
  target_variable: "pet"
  random_seed: 42

data:
  raw_data_path: "../../data/full.csv"
  time_column: "time"
  lat_column: "lat"
  lon_column: "lon"
  predictor_columns: [
      'lat', 'lon', # lat and lon are now features
      'tmp', 'dtr', 'cld', 'tmx', 
      'tmn', 'wet', 'vap', 'soi', 
      'dmi', 'pdo', 'nino4', 
      'nino34', 'nino3', 'pre', 'pet', "spei"
    ]
  train_end_date: "2017-12-31"
  validation_end_date: "2020-12-31"

feature_engineering:
  lag_periods: [1, 2, 3, 6, 9, 12]
  date_features_to_extract: ['month', 'year']

scaling:
  method: "robust"
  scaler_filename: "local_robust_scaler.joblib"

lstm_params:
  n_steps_in: 12  # Input sequence length (lookback window)
  n_steps_out: 1  # Output sequence length (forecast horizon)
  batch_size: 16

  # PyTorch Lightning Trainer settings
  trainer:
    max_epochs: 200
    patience_for_early_stopping: 10 # Early stopping patience
    accelerator: "auto" # "cpu", "gpu", "tpu", "auto"
    enable_progress_bar: True # Set to True to see progress bars for each location
  
  # Optuna Hyperparameter Tuning settings
  tuning:
    n_trials: 3  # Number of hyperparameter combinations to try for each location
    # --- Search space for each hyperparameter ---
    learning_rate:
      low: 1.0e-6
      high: 1.0e-4
      log: True
    hidden_size:
      low: 32
      high: 128
      step: 32
    n_layers:
      low: 1
      high: 3
    dropout_rate:
      low: 0.1
      high: 0.2

paths:
  models_base_dir: "../../models_saved" 

results:
  output_base_dir: "../../run_outputs" 
  metrics_filename: "all_lstm_local_models_metrics.json"
  per_location_predictions_dir: "per_location_full_predictions"
  per_location_prediction_filename_suffix: "_full_pred.csv"
