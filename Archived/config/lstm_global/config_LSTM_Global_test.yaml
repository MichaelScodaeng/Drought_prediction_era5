# config_LSTM_Global.yaml
# Configuration file for the LSTMPyTorchGlobalPipeline

project_setup:
  experiment_name: "PET_LSTM_Global_Run"
  target_variable: "pet"
  random_seed: 42

data:
  raw_data_path: "../../data/full.csv"
  time_column: "time"
  lat_column: "lat"
  lon_column: "lon"
  # Predictor columns include lat and lon for the global model
  predictor_columns: [
      'lat', 'lon', # lat and lon are now features
      'tmp', 'dtr', 'cld', 'tmx', 
      'tmn', 'wet', 'vap', 'soi', 
      'dmi', 'pdo', 'nino4', 
      'nino34', 'nino3', 'pre']
  train_end_date: "2017-12-31"      # Adjusted to give a larger validation set
  validation_end_date: "2020-12-31"

feature_engineering:
  # columns_to_lag will be auto-populated with target + all predictors (including lat/lon)
  lag_periods: [1, 2, 3, 6, 9, 12]
  date_features_to_extract: ['month', 'year']

scaling:
  method: "robust"
  scaler_filename: "global_robust_scaler.joblib" # Filename for the single global scaler

lstm_params:
  n_steps_in: 12  # Input sequence length (lookback window)
  n_steps_out: 1  # Output sequence length (forecast horizon)
  batch_size: 16 # Larger batch size for a bigger dataset

  # PyTorch Lightning Trainer settings
  trainer:
    max_epochs: 1
    patience_for_early_stopping: 10
    accelerator: "auto"
    enable_progress_bar: True # Good to have on for a single long run
  
  # Optuna Hyperparameter Tuning settings
  tuning:
    n_trials: 1 # Number of trials for the single global model
    learning_rate: {low: 1.0e-6, high: 1.0e-4, log: True}
    hidden_size: {low: 64, high: 256, step: 64}
    n_layers: {low: 2, high: 4}
    dropout_rate: {low: 0.1, high: 0.3}

paths:
  models_base_dir: "../../models_saved" 

results:
  output_base_dir: "../../run_outputs" 
  metrics_filename: "global_lstm_model_metrics.json"
  predictions_filename: "global_lstm_full_predictions.csv"
  feature_importance_filename: "shap_summary.png" # For SHAP analysis
  config_filename: "config_used_for_run.yaml"
