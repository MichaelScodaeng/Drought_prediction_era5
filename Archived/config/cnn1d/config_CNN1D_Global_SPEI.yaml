# config_CNN1D_Global.yaml
# Configuration file for the CNN1DGlobalPipeline

project_setup:
  experiment_name: "SPEI_CNN1D_Global_Run"
  target_variable: "spei"
  random_seed: 42

data:
  raw_data_path: "data/full.csv"
  time_column: "time"
  lat_column: "lat"
  lon_column: "lon"
  # Predictor columns include lat and lon for the global model
  predictor_columns: [
      'lat', 'lon',
      'tmp', 'dtr', 'cld', 'tmx', 
      'tmn', 'wet', 'vap', 'soi', 
      'dmi', 'pdo', 'nino4', 
      'nino34', 'nino3', 'pre', 'pet'
    ]
  train_end_date: "2017-12-31"
  validation_end_date: "2020-12-31"

feature_engineering:
  lag_periods: [1, 2, 3, 6, 9, 12]
  date_features_to_extract: ['month', 'year']

scaling:
  method: "robust"
  scaler_filename: "global_robust_scaler_cnn1d.joblib"

cnn1d_params:
  n_steps_in: 12      # Input sequence length (window size)
  n_steps_out: 1      # Output sequence length (forecast horizon)
  batch_size: 256

  # PyTorch Lightning Trainer settings
  trainer:
    max_epochs: 200
    patience_for_early_stopping: 5
    accelerator: "auto"
    enable_progress_bar: True
  
  # Optuna Hyperparameter Tuning settings
  tuning:
    n_trials: 1  # Number of trials for the single global model
    learning_rate: {low: 1.0e-5, high: 1.0e-5, log: True}
    # --- CNN Specific Hyperparameters ---
    n_conv_layers:
      low: 2
      high: 4
    out_channels_power: # Number of filters, expressed as a power of 2 (e.g., 32, 64, 128)
      low: 5 # 2^5 = 32
      high: 7 # 2^7 = 128
    kernel_size:
      choices: [2, 3] # Kernel sizes to try
    dropout_rate:
      low: 0.2
      high: 0.5

paths:
  models_base_dir: "models_saved" 

results:
  output_base_dir: "run_outputs" 
  metrics_filename: "global_cnn1d_model_metrics.json"
  predictions_filename: "global_cnn1d_full_predictions.csv"
  config_filename: "config_used_for_run.yaml"
