{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484f2bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Opening ERA5 Zarr dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.1 is exactly one major version older than the runtime version 6.31.1 at api.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Subsetting region and time...\n",
      "ðŸ”¹ Triggering compute and showing progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Variables:   0%|          | 0/24 [02:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m     computed_vars \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m VARIABLES:\n\u001b[1;32m---> 48\u001b[0m         computed_vars[var] \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     51\u001b[0m ds_computed \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset(computed_vars)\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\dataarray.py:1170\u001b[0m, in \u001b[0;36mDataArray.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Manually trigger loading of this array's data from disk or a\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m \u001b[38;5;124;03m    remote source into memory and return this array.\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_temp_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m     new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_temp_dataset(ds)\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable \u001b[38;5;241m=\u001b[39m new\u001b[38;5;241m.\u001b[39m_variable\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\dataset.py:870\u001b[0m, in \u001b[0;36mDataset.load\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m chunkmanager \u001b[38;5;241m=\u001b[39m get_chunked_array_type(\u001b[38;5;241m*\u001b[39mlazy_data\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    869\u001b[0m \u001b[38;5;66;03m# evaluate all the chunked arrays simultaneously\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m evaluated_data: \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, Any], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlazy_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(lazy_data, evaluated_data, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariables[k]\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\namedarray\\daskmanager.py:86\u001b[0m, in \u001b[0;36mDaskManager.compute\u001b[1;34m(self, *data, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mdata: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m     83\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray[Any, _DType_co], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute\n\u001b[1;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\dask\\base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "ZARR_PATH = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "NETCDF_OUTPUT = \"era5_europe_2021Q1.nc\"\n",
    "\n",
    "TIME_SLICE = slice(\"1959\", \"2023\")\n",
    "LAT_BOUNDS = slice(75, 30)  # Europe\n",
    "LON_BOUNDS = lambda lon: (lon >= 335) | (lon <= 50)\n",
    "\n",
    "VARIABLES = [\n",
    "    'total_precipitation_6hr',\n",
    "    '2m_temperature', '2m_dewpoint_temperature', 'surface_pressure',\n",
    "    'mean_sea_level_pressure', '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "    '10m_wind_speed', 'u_component_of_wind', 'v_component_of_wind',\n",
    "    'total_column_water_vapour', 'integrated_vapor_transport', 'boundary_layer_height',\n",
    "    'specific_humidity', 'total_cloud_cover',\n",
    "    'mean_surface_net_short_wave_radiation_flux',\n",
    "    'mean_surface_latent_heat_flux', 'mean_surface_sensible_heat_flux',\n",
    "    'snow_depth', 'sea_surface_temperature', 'volumetric_soil_water_layer_1',\n",
    "    'mean_vertically_integrated_moisture_divergence', 'eddy_kinetic_energy',\n",
    "    'land_sea_mask'\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Load and subset ZARR\n",
    "# -------------------------------\n",
    "print(\"ðŸ”¹ Opening ERA5 Zarr dataset...\")\n",
    "ds = xr.open_zarr(ZARR_PATH, consolidated=True, storage_options={\"token\": \"anon\", \"asynchronous\": False})\n",
    "\n",
    "print(\"ðŸ”¹ Subsetting region and time...\")\n",
    "ds = ds.sel(time=TIME_SLICE, latitude=LAT_BOUNDS)\n",
    "ds = ds.where(LON_BOUNDS(ds.longitude), drop=True)\n",
    "ds = ds[VARIABLES]\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Trigger computation with progress\n",
    "# -------------------------------\n",
    "print(\"ðŸ”¹ Triggering compute and showing progress...\")\n",
    "# Wrap all variables in tqdm\n",
    "with tqdm(total=len(VARIABLES), desc=\"Computing Variables\") as pbar:\n",
    "    computed_vars = {}\n",
    "    for var in VARIABLES:\n",
    "        computed_vars[var] = ds[var].load()\n",
    "        pbar.update(1)\n",
    "\n",
    "ds_computed = xr.Dataset(computed_vars)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: Save to NetCDF\n",
    "# -------------------------------\n",
    "print(f\"ðŸ’¾ Saving NetCDF to: {NETCDF_OUTPUT} ...\")\n",
    "ds_computed.to_netcdf(NETCDF_OUTPUT)\n",
    "print(\"âœ… Done. File saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ad2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (93544, 24, 181, 301)\n",
      "Target shape: (93544, 181, 301)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "class GriddedClimateDataset(Dataset):\n",
    "    def __init__(self, file_path, input_len=12, forecast_len=1, variables=None,\n",
    "                 target_variable=None, lat_bounds=(75, 30), lon_bounds=(335, 50),\n",
    "                 time_slice=slice(\"1959-01\", \"2023-01\")):\n",
    "        \n",
    "        if file_path.endswith(\".nc\"):\n",
    "            self.ds = xr.open_dataset(file_path)\n",
    "        else:\n",
    "            self.ds = xr.open_zarr(file_path, consolidated=True, decode_times=False,\n",
    "                                   storage_options={\"token\": \"anon\", \"asynchronous\": False})\n",
    "            from cftime import num2date\n",
    "            units = self.ds.time.attrs.get(\"units\", \"hours since 1900-01-01 00:00:0.0\")\n",
    "            cal = self.ds.time.attrs.get(\"calendar\", \"standard\")\n",
    "            self.ds[\"time\"] = (\"time\", num2date(self.ds.time.values, units, calendar=cal))\n",
    "\n",
    "        # Subset region and time\n",
    "        self.ds = self.ds.sel(time=time_slice, latitude=slice(*lat_bounds))\n",
    "        self.ds = self.ds.where((self.ds.longitude >= lon_bounds[0]) | \n",
    "                                (self.ds.longitude <= lon_bounds[1]), drop=True)\n",
    "\n",
    "        # Process each variable to ensure 3D (time, lat, lon)\n",
    "        processed_vars = []\n",
    "        for var in variables:\n",
    "            da = self.ds[var]\n",
    "            if \"level\" in da.dims:\n",
    "                da = da.mean(dim=\"level\")  # reduce level\n",
    "            if set(da.dims) == {\"latitude\", \"longitude\"}:  # static 2D\n",
    "                da = da.expand_dims(time=self.ds.time)\n",
    "            processed_vars.append(da)\n",
    "\n",
    "        # Stack into [time, variable, lat, lon]\n",
    "        stacked = xr.concat(processed_vars, dim=\"variable\")\n",
    "        stacked = stacked.transpose(\"time\", \"variable\", \"latitude\", \"longitude\")\n",
    "\n",
    "        self.data = stacked\n",
    "        self.target = self.ds[target_variable].transpose(\"time\", \"latitude\", \"longitude\")\n",
    "        self.time_coords = self.ds.time.values\n",
    "        self.input_len = input_len\n",
    "        self.forecast_len = forecast_len\n",
    "        self.variables = variables\n",
    "        self.target_variable = target_variable\n",
    "        self.length = len(self.data.time) - input_len - forecast_len + 1\n",
    "\n",
    "\n",
    "\n",
    "# --- Example Training Loop for ConvLSTM / MESA-NET Compatible ---\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)  # Expected output shape: [B, forecast_len, H, W]\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Usage\n",
    "INPUT_LEN = 12  # Number of input time steps\n",
    "FORECAST_LEN = 4  # Number of forecast time steps\n",
    "PATH = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "VARIABLES = [\n",
    "    'total_precipitation_6hr',\n",
    "    '2m_temperature', '2m_dewpoint_temperature', 'surface_pressure',\n",
    "    'mean_sea_level_pressure', '10m_u_component_of_wind', '10m_v_component_of_wind',\n",
    "    '10m_wind_speed', 'u_component_of_wind', 'v_component_of_wind',\n",
    "    'total_column_water_vapour', 'integrated_vapor_transport', 'boundary_layer_height',\n",
    "    'specific_humidity', 'total_cloud_cover',\n",
    "    'mean_surface_net_short_wave_radiation_flux',\n",
    "    'mean_surface_latent_heat_flux', 'mean_surface_sensible_heat_flux',\n",
    "    'snow_depth', 'sea_surface_temperature', 'volumetric_soil_water_layer_1',\n",
    "    'mean_vertically_integrated_moisture_divergence', 'eddy_kinetic_energy',\n",
    "    'land_sea_mask'\n",
    "]\n",
    "TARGET_VARIABLE = 'total_precipitation_6hr'\n",
    "dataset = GriddedClimateDataset(PATH, input_len=INPUT_LEN, forecast_len=FORECAST_LEN, \n",
    "                                  variables=VARIABLES, target_variable=TARGET_VARIABLE)\n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "# model = YourConvLSTMModel(...).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# criterion = torch.nn.MSELoss()\n",
    "print(\"Data shape:\", dataset.data.shape)\n",
    "print(\"Target shape:\", dataset.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6743df5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Subclasses of Dataset should implement __getitem__.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample x shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# should be [input_len, num_vars, H, W]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample y shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# should be [forecast_len, H, W] â†’ [6, 181, 301]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:59\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T_co:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubclasses of Dataset should implement __getitem__.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Subclasses of Dataset should implement __getitem__."
     ]
    }
   ],
   "source": [
    "x, y = dataset[0]\n",
    "print(\"Sample x shape:\", x.shape)  # should be [input_len, num_vars, H, W]\n",
    "print(\"Sample y shape:\", y.shape)  # should be [forecast_len, H, W] â†’ [6, 181, 301]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
