{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4870ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory of the notebook\n",
    "notebook_dir = os.getcwd()\n",
    "# Go one level up to get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "\n",
    "# Add the project root to the Python path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Now you should be able to import your modules\n",
    "# from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "# from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "# from src.feature_utils import engineer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fdda90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not import from data_utils_v1 for __main__ block. Ensure it's accessible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib # To load the scaler if needed for inverse transform verification\n",
    "\n",
    "# Your utility functions (make sure your VS Code/Jupyter is set up to find the 'src' directory)\n",
    "# If running the notebook from the project root, these imports should work:\n",
    "from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "from src.feature_utils import engineer_features\n",
    "\n",
    "#reimport\n",
    "from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2683d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.1 is exactly one major version older than the runtime version 6.31.1 at api.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset time range: 1959-01-01T00:00:00.000000000 to 2023-01-10T18:00:00.000000000\n",
      "Dataset loaded: {'time': 360, 'latitude': 181, 'longitude': 301, 'level': 13}\n",
      "Time range: 2021-01-01T00:00:00.000000000 to 2021-03-31T18:00:00.000000000\n",
      "Available variables: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_49256\\1131974750.py:24: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"Dataset loaded: {dict(ds.dims)}\")\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_zarr(\n",
    "            \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\",\n",
    "            consolidated=True,\n",
    "            storage_options={\"token\": \"anon\", \"asynchronous\": False}\n",
    "        )\n",
    "#get start and end time\n",
    "start_time = ds.time.values[0]\n",
    "end_time = ds.time.values[-1]\n",
    "# Print the start and end time\n",
    "print(f\"Dataset time range: {start_time} to {end_time}\")\n",
    "        \n",
    "# Select time range first\n",
    "ds = ds.sel(time=slice(\"2021-01\", \"2021-03\"))\n",
    "\n",
    "# Apply Europe bounds (handle longitude wrapping)\n",
    "# Apply Europe bounds with consistent selection\n",
    "europe_mask = (ds.longitude >= 335) | (ds.longitude <= 50)\n",
    "ds = ds.where(europe_mask, drop=True).sel(latitude=slice(75, 30))\n",
    "\n",
    "# Ensure consistent longitude selection\n",
    "target_lons = ds.longitude.values\n",
    "\n",
    "print(f\"Dataset loaded: {dict(ds.dims)}\")\n",
    "print(f\"Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "print(f\"Available variables: {len(list(ds.data_vars.keys()))}\")\n",
    "variables = [\n",
    "    # TARGET VARIABLE\n",
    "    'total_precipitation_6hr',              # Our main target\n",
    "\n",
    "    # CORE ATMOSPHERIC VARIABLES\n",
    "    '2m_temperature',                       # Surface temperature\n",
    "    '2m_dewpoint_temperature',              # Surface moisture content\n",
    "    'surface_pressure',                     # Surface pressure\n",
    "    'mean_sea_level_pressure',              # Synoptic pressure patterns\n",
    "\n",
    "    # WIND FIELDS\n",
    "    '10m_u_component_of_wind',              # Surface wind U\n",
    "    '10m_v_component_of_wind',              # Surface wind V\n",
    "    '10m_wind_speed',                       # Surface wind magnitude\n",
    "    'u_component_of_wind',                  # Upper-level winds (averaged over pressure levels)\n",
    "    'v_component_of_wind',                  # Upper-level winds (averaged over pressure levels)\n",
    "\n",
    "    # MOISTURE & THERMODYNAMICS\n",
    "    'total_column_water_vapour',            # Atmospheric moisture content\n",
    "    'integrated_vapor_transport',           # Moisture transport\n",
    "    'boundary_layer_height',                # PBL structure\n",
    "    'specific_humidity',                    # Atmospheric humidity (averaged over pressure levels)\n",
    "\n",
    "    # CLOUDS & RADIATION\n",
    "    'total_cloud_cover',                    # Cloud coverage\n",
    "    'mean_surface_net_short_wave_radiation_flux',  # Solar heating\n",
    "    'mean_surface_latent_heat_flux',        # Evaporation\n",
    "    'mean_surface_sensible_heat_flux',      # Surface heating\n",
    "\n",
    "    # SURFACE CONDITIONS\n",
    "    'snow_depth',                           # Snow coverage\n",
    "    'sea_surface_temperature',              # SST for coastal areas\n",
    "    'volumetric_soil_water_layer_1',        # Surface soil moisture\n",
    "\n",
    "    # ATMOSPHERIC DYNAMICS\n",
    "    'mean_vertically_integrated_moisture_divergence',  # Moisture convergence\n",
    "    'eddy_kinetic_energy',                  # Turbulence measure\n",
    "    \n",
    "    #2D VARIABLES\n",
    "    'land_sea_mask',\n",
    "    \n",
    "\n",
    "]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a6cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Class: Successfully imported utility functions.\n",
      "Pipeline Class: Successfully imported utility functions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt # For feature importance\n",
    "import json\n",
    "# Assuming these are in src/ or PYTHONPATH is set for the notebook\n",
    "try:\n",
    "    from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "    from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "    from src.feature_utils import engineer_features\n",
    "    print(\"Pipeline Class: Successfully imported utility functions.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pipeline Class Error: Could not import utility functions: {e}\")\n",
    "    print(\"Ensure your PYTHONPATH is set correctly if running from a notebook, or that src is accessible.\")\n",
    "    # Define dummy functions if import fails, so class can be parsed\n",
    "    def load_config(path=None): return {}\n",
    "    def load_and_prepare_data(config=None): return None\n",
    "    def split_data_chronologically(df=None, config=None): return None, None, None\n",
    "    def engineer_features(df=None, config=None): return df\n",
    "    def scale_data(df_train=None, df_val=None, df_test=None, config=None): return None,None,None,None\n",
    "    def save_scaler(scaler=None, path=None): pass\n",
    "    def load_scaler(path=None): return None\n",
    "    def inverse_transform_predictions(df=None, target=None, scaler=None): return None\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt # For feature importance\n",
    "import json # For saving metrics\n",
    "\n",
    "# Assuming these are in src/ or PYTHONPATH is set for the notebook\n",
    "try:\n",
    "    from src.data_utils import load_config, load_and_prepare_data, split_data_chronologically\n",
    "    from src.preprocess_utils import scale_data, save_scaler, load_scaler, inverse_transform_predictions\n",
    "    from src.feature_utils import engineer_features\n",
    "    print(\"Pipeline Class: Successfully imported utility functions.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pipeline Class Error: Could not import utility functions: {e}\")\n",
    "    print(\"Ensure your PYTHONPATH is set correctly if running from a notebook, or that src is accessible.\")\n",
    "    # Define dummy functions if import fails, so class can be parsed\n",
    "    def load_config(path=None): return {}\n",
    "    def load_and_prepare_data(config=None): return None\n",
    "    def split_data_chronologically(df=None, config=None): return None, None, None\n",
    "    def engineer_features(df=None, config=None): return df\n",
    "    def scale_data(df_train=None, df_val=None, df_test=None, config=None): return None,None,None,None\n",
    "    def save_scaler(scaler=None, path=None): pass\n",
    "    def load_scaler(path=None): return None\n",
    "    def inverse_transform_predictions(df=None, target=None, scaler=None): return None\n",
    "\n",
    "\n",
    "class XGBoostGlobalPipeline:\n",
    "    def __init__(self, config_path=\"config.yaml\"):\n",
    "        self.config_path_abs = os.path.abspath(config_path)\n",
    "        print(f\"Pipeline Class: Attempting to load config from: {self.config_path_abs}\")\n",
    "        self.cfg = load_config(self.config_path_abs)\n",
    "        \n",
    "        if not self.cfg or self.cfg.get('data',{}).get('raw_data_path') is None:\n",
    "            print(\"Pipeline Class Warning: Configuration might not have loaded correctly. Critical paths might be missing.\")\n",
    "\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        self.best_hyperparams = None\n",
    "        self.train_df_raw, self.val_df_raw, self.test_df_raw = None, None, None\n",
    "        self.train_df_featured, self.val_df_featured, self.test_df_featured = None, None, None\n",
    "        self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test = [None]*6\n",
    "        self.full_df_raw_for_prediction = None # Initialize\n",
    "        \n",
    "        self.experiment_name = self.cfg.get('project_setup', {}).get('project_name', 'default_experiment')\n",
    "        self.project_root_for_paths = os.path.dirname(self.config_path_abs) # Directory of config file\n",
    "\n",
    "        results_base_cfg = self.cfg.get('paths',{}).get('output_base_dir', 'run_outputs')\n",
    "        self.run_output_dir = os.path.join(self.project_root_for_paths, results_base_cfg, self.experiment_name)\n",
    "        \n",
    "        models_base_dir_cfg = self.cfg.get('paths', {}).get('models_base_dir', 'models_saved') # Changed to 'paths.models_base_dir'\n",
    "        self.run_models_dir = os.path.join(self.project_root_for_paths, models_base_dir_cfg, self.experiment_name)\n",
    "\n",
    "        os.makedirs(self.run_output_dir, exist_ok=True)\n",
    "        os.makedirs(self.run_models_dir, exist_ok=True)\n",
    "        print(f\"Pipeline Class: Artifacts for experiment '{self.experiment_name}' will be saved under '{self.run_output_dir}' and '{self.run_models_dir}'\")\n",
    "\n",
    "\n",
    "    def _get_abs_path_from_config_value(self, relative_path_from_config_value):\n",
    "        if relative_path_from_config_value is None: return None\n",
    "        if os.path.isabs(relative_path_from_config_value): return relative_path_from_config_value\n",
    "        return os.path.abspath(os.path.join(self.project_root_for_paths, relative_path_from_config_value))\n",
    "\n",
    "    def load_and_split_data(self):\n",
    "        print(\"Pipeline: Loading and splitting data...\")\n",
    "        relative_raw_data_path = self.cfg.get('data', {}).get('raw_data_path')\n",
    "        if not relative_raw_data_path:\n",
    "            print(\"Pipeline Error: 'data.raw_data_path' not found in configuration.\")\n",
    "            return\n",
    "        abs_data_file_path = self._get_abs_path_from_config_value(relative_raw_data_path)\n",
    "        if not abs_data_file_path or not os.path.exists(abs_data_file_path):\n",
    "            print(f\"Pipeline Error: Data file not found at constructed absolute path: {abs_data_file_path}\")\n",
    "            return\n",
    "\n",
    "        temp_load_cfg = self.cfg.copy(); temp_load_cfg['data'] = self.cfg['data'].copy() \n",
    "        temp_load_cfg['data']['raw_data_path'] = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "        temp_load_cfg['data']['start_date'] = \"2021-01-01\"\n",
    "        temp_load_cfg['data']['end_date'] = \"2021-03-31\"\n",
    "        temp_load_cfg['data']['lon_bounds'] = [75, 30]\n",
    "        full_df_raw = load_and_prepare_data(temp_load_cfg) \n",
    "        if full_df_raw is None:\n",
    "            print(\"Pipeline Error: data_utils.load_and_prepare_data returned None.\"); return\n",
    "        self.full_df_raw_for_prediction = full_df_raw.copy() \n",
    "        self.train_df_raw, self.val_df_raw, self.test_df_raw = split_data_chronologically(full_df_raw, self.cfg)\n",
    "        print(f\"Pipeline: Data loaded and split. Train shape: {self.train_df_raw.shape if self.train_df_raw is not None else 'None'}\")\n",
    "\n",
    "    def engineer_all_features(self):\n",
    "        print(\"Pipeline: Engineering features...\")\n",
    "        if self.train_df_raw is None: raise ValueError(\"Raw training data not loaded for feature engineering.\")\n",
    "        self.train_df_featured = engineer_features(self.train_df_raw.copy(), self.cfg)\n",
    "        self.val_df_featured = engineer_features(self.val_df_raw.copy(), self.cfg)\n",
    "        self.test_df_featured = engineer_features(self.test_df_raw.copy(), self.cfg)\n",
    "        #print shape all\n",
    "        print(f\"Pipeline: Feature engineering complete. Train shape: {self.train_df_featured.shape if self.train_df_featured is not None else 'None'}, \"\n",
    "                f\"Validation shape: {self.val_df_featured.shape if self.val_df_featured is not None else 'None'}, \"\n",
    "                f\"Test shape: {self.test_df_featured.shape if self.test_df_featured is not None else 'None'}\")\n",
    "\n",
    "    def preprocess_all_data(self):\n",
    "        print(\"Pipeline: Scaling data...\")\n",
    "        if self.train_df_featured is None: raise ValueError(\"Featured training data not available for scaling.\")\n",
    "        scaled_train, scaled_val, scaled_test, fitted_sclr = scale_data(\n",
    "            self.train_df_featured, self.val_df_featured, self.test_df_featured, self.cfg)\n",
    "        if fitted_sclr is None: raise ValueError(\"Scaler fitting failed.\")\n",
    "        self.scaler = fitted_sclr\n",
    "        \n",
    "        target_col = self.cfg['project_setup']['target_variable']\n",
    "        time_col = self.cfg['data']['time_column']\n",
    "        cols_to_drop_for_X = [target_col]\n",
    "        if time_col in scaled_train.columns: cols_to_drop_for_X.append(time_col)\n",
    "        #drop any columns that are not contains \"lag\" or \"rolling\" in their name\n",
    "        cols_to_drop_for_X += [col for col in scaled_train.columns if 'lag' not in col and 'rolling' not in col and col!=\"year\" and col!=\"month\" and col!=\"day\" and col!=\"lat\" and col!=\"lon\"]\n",
    "        self.X_train = scaled_train.drop(columns=cols_to_drop_for_X, errors='ignore')\n",
    "        print(\"Columns: \", self.X_train.columns.tolist()) # DEBUG PRINT\n",
    "        self.y_train = scaled_train[target_col]\n",
    "        self.X_val = scaled_val.drop(columns=cols_to_drop_for_X, errors='ignore')\n",
    "        self.y_val = scaled_val[target_col]\n",
    "        self.X_test = scaled_test.drop(columns=cols_to_drop_for_X, errors='ignore')\n",
    "        self.y_test = scaled_test[target_col]\n",
    "\n",
    "\n",
    "        scaler_filename = self.cfg.get('scaling',{}).get('scaler_filename', 'robust_scaler.joblib')\n",
    "        scaler_save_path = os.path.join(self.run_models_dir, scaler_filename) \n",
    "        save_scaler(self.scaler, scaler_save_path)\n",
    "        print(f\"Pipeline: Data scaling and X,y preparation complete. Scaler saved to {scaler_save_path}\")\n",
    "\n",
    "    def _objective_for_optuna(self, trial):\n",
    "        target_col = self.cfg['project_setup']['target_variable']\n",
    "        param = {\n",
    "            'objective': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('objective', 'reg:squarederror'),\n",
    "            'eval_metric': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('eval_metric', 'rmse'),\n",
    "            'tree_method': 'hist', 'random_state': self.cfg.get('project_setup', {}).get('random_seed', 42),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=100),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        }\n",
    "        model = xgb.XGBRegressor(**param,early_stopping_rounds = 2)\n",
    "        fit_params_opt = {'verbose': False}\n",
    "        if xgb.__version__ >= '0.90': \n",
    "            fit_params_opt['eval_set'] = [(self.X_val, self.y_val)] \n",
    "        model.fit(self.X_train, self.y_train, **fit_params_opt) \n",
    "        preds_val_scaled = model.predict(self.X_val) \n",
    "        scaled_preds_val_df_opt = pd.DataFrame(preds_val_scaled, columns=[target_col], index=self.X_val.index)\n",
    "        inversed_predictions_val_opt = inverse_transform_predictions(scaled_preds_val_df_opt, target_col, self.scaler)\n",
    "        scaled_actuals_val_df_opt = pd.DataFrame(self.y_val.values, columns=[target_col], index=self.y_val.index)\n",
    "        inversed_actuals_val_opt = inverse_transform_predictions(scaled_actuals_val_df_opt, target_col, self.scaler)\n",
    "        if inversed_predictions_val_opt is None or inversed_actuals_val_opt is None: return float('inf')\n",
    "        return mean_squared_error(inversed_actuals_val_opt, inversed_predictions_val_opt)\n",
    "\n",
    "    def tune_hyperparameters(self, n_trials=50):\n",
    "        print(\"Pipeline: Tuning hyperparameters...\")\n",
    "        if self.X_train is None: raise ValueError(\"Data not preprocessed for hyperparameter tuning.\")\n",
    "        print(f\"Pipeline: Starting hyperparameter tuning with {n_trials} trials...\")\n",
    "        print(f\"Pipeline: Using {self.X_train.shape} training samples, {self.X_val.shape} validation samples.\")\n",
    "        print(\"Columns in X_train:\", self.X_train.columns.tolist()) # DEBUG PRINT\n",
    "        print(\"Columns in X_val:\", self.X_val.columns.tolist())\n",
    "        print(\"Target variable:\", self.cfg['project_setup']['target_variable']) # DEBUG PRINT\n",
    "        print(\" in X_train:\", self.X_train.columns.tolist()) # DEBUG PRINT\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(self._objective_for_optuna, n_trials=n_trials)\n",
    "        self.best_hyperparams = study.best_trial.params\n",
    "        print(f\"Pipeline: Hyperparameter tuning complete. Best RMSE on validation: {study.best_trial.value:.4f}\")\n",
    "        print(f\"Best params: {self.best_hyperparams}\")\n",
    "\n",
    "    def train_final_model(self, params=None):\n",
    "        print(\"Pipeline: Training final model...\")\n",
    "        if self.X_train is None: raise ValueError(\"Data not preprocessed for final model training.\")\n",
    "        model_params_to_use = params if params else self.best_hyperparams\n",
    "        if not model_params_to_use:\n",
    "            print(\"Pipeline Warning: No best hyperparameters. Using initial defaults from config.\")\n",
    "            model_params_to_use = self.cfg.get('model_params', {}).get('global_xgboost', {}).copy(); model_params_to_use.pop('tuning', None) \n",
    "        final_xgb_model_params = {\n",
    "            'objective': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('objective', 'reg:squarederror'),\n",
    "            'eval_metric': self.cfg.get('model_params', {}).get('global_xgboost', {}).get('eval_metric', 'rmse'),\n",
    "            'tree_method': 'hist', 'random_state': self.cfg.get('project_setup', {}).get('random_seed', 42),\n",
    "            **model_params_to_use }\n",
    "        self.model = xgb.XGBRegressor(**final_xgb_model_params)\n",
    "        print(f\"Training final model on X_train (shape: {self.X_train.shape})\")\n",
    "        #remove target from X_train if it exists\n",
    "        print(self.X_train.columns.tolist()) # DEBUG PRINT\n",
    "        print(\"========== DEBUG: X_train Head ==========\")\n",
    "        print(self.X_train.head())\n",
    "        print(\"X_train shape:\", self.X_train.shape)\n",
    "        print(\"X_train columns:\", self.X_train.columns.tolist())\n",
    "\n",
    "        print(\"\\n========== DEBUG: y_train Head ==========\")\n",
    "        print(self.y_train.head())\n",
    "        print(\"y_train shape:\", self.y_train.shape)\n",
    "        print(\"y_train descriptive stats:\")\n",
    "        print(self.y_train.describe())\n",
    "\n",
    "        # Also check for NaNs\n",
    "        print(\"\\n========== DEBUG: Check NaNs ==========\")\n",
    "        print(\"X_train has NaNs:\", self.X_train.isna().sum().sum() > 0)\n",
    "        print(\"y_train has NaNs:\",self.y_train.isna().sum() > 0)\n",
    "        print(\"full shape\", self.X_train.shape, \"y shape:\", self.y_train.shape) # DEBUG PRINT\n",
    "\n",
    "        self.model.fit(self.X_train, self.y_train, verbose=False) \n",
    "        self.save_model() \n",
    "        print(\"Pipeline: Final model trained and saved.\")\n",
    "\n",
    "    def save_model(self):\n",
    "        if self.model is None: print(\"Pipeline Error: No model to save.\"); return\n",
    "        model_filename = self.cfg.get('model_params',{}).get('global_xgboost',{}).get('model_filename', 'xgboost_model.json')\n",
    "        model_save_path = os.path.join(self.run_models_dir, model_filename)\n",
    "        try:\n",
    "            self.model.save_model(model_save_path) \n",
    "            print(f\"Pipeline: XGBoost model saved to {model_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not save XGBoost model to {model_save_path}: {e}\")\n",
    "\n",
    "    def evaluate(self, data_split='test'):\n",
    "        print(f\"Pipeline: Evaluating model on {data_split} set...\")\n",
    "        if self.model is None: print(\"Pipeline Error: Model not trained.\"); return None\n",
    "        if self.scaler is None: print(\"Pipeline Error: Scaler not available.\"); return None\n",
    "\n",
    "        X_eval, y_eval_scaled = None, None\n",
    "        if data_split == 'test' and self.X_test is not None: X_eval, y_eval_scaled = self.X_test, self.y_test\n",
    "        elif data_split == 'validation' and self.X_val is not None: X_eval, y_eval_scaled = self.X_val, self.y_val\n",
    "        elif data_split == 'train' and self.X_train is not None: X_eval, y_eval_scaled = self.X_train, self.y_train\n",
    "        else: print(f\"Pipeline Error: Data for split '{data_split}' unavailable.\"); return None\n",
    "        \n",
    "        scaled_predictions = self.model.predict(X_eval)\n",
    "        target_col = self.cfg['project_setup']['target_variable']\n",
    "        scaled_actuals_df = pd.DataFrame(y_eval_scaled.values, columns=[target_col], index=y_eval_scaled.index)\n",
    "        scaled_preds_df = pd.DataFrame(scaled_predictions, columns=[target_col], index=y_eval_scaled.index)\n",
    "        inversed_predictions = inverse_transform_predictions(scaled_preds_df, target_col, self.scaler)\n",
    "        inversed_actuals = inverse_transform_predictions(scaled_actuals_df, target_col, self.scaler)\n",
    "        \n",
    "        if inversed_predictions is not None and inversed_actuals is not None:\n",
    "            from sklearn.metrics import root_mean_squared_error\n",
    "            rmse = root_mean_squared_error(inversed_actuals, inversed_predictions)\n",
    "            mae = mean_absolute_error(inversed_actuals, inversed_predictions)\n",
    "            r2 = r2_score(inversed_actuals, inversed_predictions)\n",
    "            print(f\"{data_split.capitalize()} Set Evaluation (Original Scale): RMSE={rmse:.4f}, MAE={mae:.4f}, R2={r2:.4f}\")\n",
    "            return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "        else: print(f\"Pipeline Error: Could not inverse transform {data_split} predictions/actuals.\"); return None\n",
    "\n",
    "    def generate_and_save_feature_importance(self):\n",
    "        if self.model is None or not hasattr(self.model, 'feature_importances_'):\n",
    "            print(\"Pipeline Warning: Model not trained or doesn't support feature importance. Skipping plot.\")\n",
    "            return\n",
    "        if self.X_train is None or self.X_train.empty:\n",
    "            print(\"Pipeline Warning: X_train is not available. Cannot map feature importances to names. Skipping plot.\")\n",
    "            return\n",
    "\n",
    "        feat_imp_filename = self.cfg.get('results',{}).get('feature_importance_filename', 'feature_importance.png')\n",
    "        plot_save_path = os.path.join(self.run_output_dir, feat_imp_filename)\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(10, max(8, len(self.X_train.columns) * 0.25))) \n",
    "            xgb.plot_importance(self.model, ax=ax, max_num_features=20, height=0.8, importance_type='weight') \n",
    "            ax.set_title(f\"XGBoost Feature Importance ({self.experiment_name})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(plot_save_path)\n",
    "            plt.close(fig) \n",
    "            print(f\"Pipeline: Feature importance plot saved to {plot_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not generate/save feature importance plot: {e}\")\n",
    "\n",
    "    def predict_on_full_data(self):\n",
    "        print(\"Pipeline: Generating predictions on the full raw dataset...\")\n",
    "        if self.model is None or self.scaler is None:\n",
    "            print(\"Pipeline Error: Model or scaler not available. Cannot make full data predictions.\")\n",
    "            return None\n",
    "        if self.full_df_raw_for_prediction is None: \n",
    "            print(\"Pipeline Error: Original full raw dataframe copy not available for prediction.\")\n",
    "            return None\n",
    "\n",
    "        print(\"  Engineering features for full dataset...\")\n",
    "        self.full_df_raw_for_prediction.sort_values(by=self.cfg['data']['time_column'], inplace=True) # Ensure time order\n",
    "        full_df_featured = engineer_features(self.full_df_raw_for_prediction.copy(), self.cfg)\n",
    "        self.full_df_raw_for_prediction = full_df_featured.copy() # Update the original copy with featured data\n",
    "        if full_df_featured.empty:\n",
    "            print(\"Pipeline Error: Feature engineering on full dataset resulted in an empty DataFrame.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"  Columns in full_df_featured after engineering: {full_df_featured.columns.tolist()}\") # DEBUG PRINT\n",
    "\n",
    "        time_col = self.cfg['data']['time_column']\n",
    "        target_col_name = self.cfg['project_setup']['target_variable']\n",
    "        \n",
    "        scaler_feature_names = list(self.scaler.feature_names_in_) if hasattr(self.scaler, 'feature_names_in_') else []\n",
    "        if not scaler_feature_names:\n",
    "            print(\"Pipeline Error: Scaler has no feature_names_in_. Was it fitted correctly on named features?\")\n",
    "            return None\n",
    "\n",
    "        # Create a DataFrame with only the columns the scaler was fitted on, in that order\n",
    "        df_to_scale_full = pd.DataFrame(index=full_df_featured.index)\n",
    "        for col in scaler_feature_names:\n",
    "            if col in full_df_featured:\n",
    "                df_to_scale_full[col] = full_df_featured[col]\n",
    "            else:\n",
    "                # This means a column the scaler expects is missing after feature engineering the full data.\n",
    "                print(f\"Pipeline Warning: Column '{col}' (expected by scaler) not found in feature-engineered full data. Filling with NaN.\")\n",
    "                df_to_scale_full[col] = np.nan # Scaler might handle NaNs (e.g. RobustScaler ignores them) or fail.\n",
    "\n",
    "        print(\"  Scaling features for full dataset...\")\n",
    "        scaled_values_for_subset = self.scaler.transform(df_to_scale_full[scaler_feature_names])\n",
    "        scaled_subset_df = pd.DataFrame(scaled_values_for_subset, columns=scaler_feature_names, index=df_to_scale_full.index)\n",
    "\n",
    "        # Now, construct X_full_for_prediction using self.X_train.columns as the template\n",
    "        # It should contain:\n",
    "        # 1. Scaled versions of columns that were in scaler_feature_names\n",
    "        # 2. Original (unscaled) versions of other columns that are in X_train.columns (e.g. lat, lon, month, year)\n",
    "        \n",
    "        X_full_for_prediction = pd.DataFrame(index=full_df_featured.index)\n",
    "        print(f\"  Model expects columns: {self.X_train.columns.tolist()}\") # DEBUG PRINT\n",
    "\n",
    "        for col in self.X_train.columns:\n",
    "            if col in scaled_subset_df.columns: # If it was a column that got scaled\n",
    "                X_full_for_prediction[col] = scaled_subset_df[col]\n",
    "            elif col in full_df_featured.columns: # If it's an unscaled feature (like lat, lon, month, year)\n",
    "                X_full_for_prediction[col] = full_df_featured[col]\n",
    "            else:\n",
    "                print(f\"Pipeline CRITICAL Warning: Feature '{col}' expected by model not found in any processed full data source. Filling with 0.\")\n",
    "                X_full_for_prediction[col] = 0 # Fallback: not ideal\n",
    "\n",
    "        print(f\"  Shape of X_full_for_prediction before predict: {X_full_for_prediction.shape}\")\n",
    "        print(f\"  Columns in X_full_for_prediction before predict: {X_full_for_prediction.columns.tolist()}\") # DEBUG PRINT\n",
    "\n",
    "        print(\"  Making predictions...\")\n",
    "        scaled_predictions_full = self.model.predict(X_full_for_prediction)\n",
    "\n",
    "        print(\"  Inverse transforming predictions...\")\n",
    "        scaled_preds_full_df = pd.DataFrame(scaled_predictions_full, columns=[target_col_name], index=full_df_featured.index)\n",
    "        inversed_predictions_full = inverse_transform_predictions(scaled_preds_full_df, target_col_name, self.scaler)\n",
    "\n",
    "        if inversed_predictions_full is not None:\n",
    "            # Start with original full_df_raw_for_prediction to keep original columns and correct length before feature engineering NaNs were dropped\n",
    "            # Then merge predictions based on index.\n",
    "            # The index of inversed_predictions_full matches full_df_featured (after NaN drop).\n",
    "            # So, we need to add predictions to full_df_featured first, then decide what to merge back to the true original.\n",
    "            \n",
    "            output_df_with_predictions = full_df_featured.copy()\n",
    "            output_df_with_predictions[f'{target_col_name}_predicted'] = inversed_predictions_full.values # .values to align if index is slightly off\n",
    "\n",
    "            # What to save? We want original time, lat, lon, original spei (if available), and predicted spei.\n",
    "            # The full_df_raw_for_prediction has the original length and all original data.\n",
    "            # We can merge our predictions (which are on the reduced length full_df_featured index) back to full_df_raw_for_prediction.\n",
    "            \n",
    "            final_output_df = self.full_df_raw_for_prediction.copy()\n",
    "            # Add the prediction where indexes match. Non-matching will be NaN.\n",
    "            final_output_df = final_output_df.merge(\n",
    "                output_df_with_predictions[[f'{target_col_name}_predicted']], # Only the prediction column\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                how='left' # Keep all original rows, add predictions where available\n",
    "            )\n",
    "\n",
    "\n",
    "            cols_to_save = [time_col, 'lat', 'lon']\n",
    "            if target_col_name in final_output_df.columns: \n",
    "                cols_to_save.append(target_col_name)\n",
    "            cols_to_save.append(f'{target_col_name}_predicted')\n",
    "            for orig_pred_col in ['pre','tmp']: # Example other original columns\n",
    "                if orig_pred_col in final_output_df.columns:\n",
    "                     cols_to_save.append(orig_pred_col)\n",
    "            \n",
    "            final_output_df_subset = final_output_df[[col for col in cols_to_save if col in final_output_df.columns]]\n",
    "\n",
    "            pred_filename = self.cfg.get('results',{}).get('predictions_filename', 'full_data_predictions.csv')\n",
    "            save_path = os.path.join(self.run_output_dir, pred_filename)\n",
    "            try:\n",
    "                final_output_df_subset.to_csv(save_path, index=False)\n",
    "                print(f\"Pipeline: Full data predictions saved to {save_path}\")\n",
    "                return final_output_df_subset\n",
    "            except Exception as e:\n",
    "                print(f\"Pipeline Error: Could not save full data predictions: {e}\")\n",
    "        else:\n",
    "            print(\"Pipeline Error: Failed to inverse transform full data predictions.\")\n",
    "        return None\n",
    "\n",
    "    def save_run_config(self):\n",
    "        config_filename = self.cfg.get('results',{}).get('config_filename', 'config_used.yaml')\n",
    "        save_path = os.path.join(self.run_output_dir, config_filename)\n",
    "        try:\n",
    "            with open(save_path, 'w') as f:\n",
    "                yaml.dump(self.cfg, f, default_flow_style=False, sort_keys=False)\n",
    "            print(f\"Pipeline: Configuration used for this run saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not save run configuration: {e}\")\n",
    "\n",
    "    def run_full_pipeline(self, tune=True, n_trials_tuning=200):\n",
    "        print(f\"\\n--- Starting Pipeline Run: Experiment '{self.experiment_name}' ---\")\n",
    "        self.load_and_split_data()\n",
    "        if self.train_df_raw is None: print(\"Pipeline Halted: Failed at data loading/splitting.\"); return \"Failed: Data Load/Split\"\n",
    "        \n",
    "        self.engineer_all_features()\n",
    "        if self.train_df_featured is None or self.train_df_featured.empty : print(\"Pipeline Halted: Failed at feature engineering.\"); return \"Failed: Feature Engineering\"\n",
    "        \n",
    "        self.preprocess_all_data()\n",
    "        if self.X_train is None: print(\"Pipeline Halted: Failed at data preprocessing/scaling.\"); return \"Failed: Preprocessing\"\n",
    "\n",
    "        if tune:\n",
    "            self.tune_hyperparameters(n_trials=n_trials_tuning)\n",
    "        \n",
    "        self.train_final_model() \n",
    "        if self.model is None: print(\"Pipeline Halted: Failed at final model training.\"); return \"Failed: Model Training\"\n",
    "\n",
    "        self.generate_and_save_feature_importance()\n",
    "        self.save_run_config() \n",
    "\n",
    "        all_metrics = {}\n",
    "        print(\"\\n--- Final Model Evaluation ---\")\n",
    "        for split_name in ['train', 'validation', 'test']:\n",
    "            metrics = self.evaluate(data_split=split_name)\n",
    "            if metrics: all_metrics[split_name] = metrics\n",
    "        \n",
    "        metrics_filename = self.cfg.get('results',{}).get('metrics_filename', 'evaluation_metrics.json')\n",
    "        metrics_save_path = os.path.join(self.run_output_dir, metrics_filename)\n",
    "        try:\n",
    "            with open(metrics_save_path, 'w') as f:\n",
    "                json.dump(all_metrics, f, indent=4) \n",
    "            print(f\"Pipeline: Evaluation metrics saved to {metrics_save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline Error: Could not save metrics: {e}\")\n",
    "\n",
    "        self.predict_on_full_data() \n",
    "\n",
    "        print(f\"--- Pipeline Run Finished: Experiment '{self.experiment_name}' ---\")\n",
    "        return all_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the XGBoost global pipeline for PRE...\n",
      "Pipeline Class: Attempting to load config from: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_test.yaml\n",
      "Configuration loaded from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_test.yaml\n",
      "Pipeline Class: Artifacts for experiment 'test_Forecasting_Global_XGBoost' will be saved under 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../run_outputs\\test_Forecasting_Global_XGBoost' and 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../models_saved\\test_Forecasting_Global_XGBoost'\n",
      "Starting the XGBoost global pipeline execution...\n",
      "\n",
      "--- Starting Pipeline Run: Experiment 'test_Forecasting_Global_XGBoost' ---\n",
      "Pipeline: Loading and splitting data...\n",
      "Loading data from: gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the XGBoost global pipeline for PRE...\")\n",
    "\n",
    "config_file_for_pipeline = \"../config/xgb_global/config_XGBoostGlobal_test.yaml\"\n",
    "pipeline = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "\n",
    "print(\"Starting the XGBoost global pipeline execution...\")\n",
    "# This single call now handles everything, including generating and saving the full predictions.\n",
    "results = pipeline.run_full_pipeline(tune=True, n_trials_tuning=1) \n",
    "print(\"Pipeline execution completed. Results:\", results)\n",
    "\n",
    "# You can load the saved predictions file if you need to inspect it.\n",
    "# The path is defined in your config under results.predictions_filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the XGBoost global pipeline for PRE...\n",
      "Pipeline Class: Attempting to load config from: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_PRE.yaml\n",
      "Configuration loaded from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_PRE.yaml\n",
      "Pipeline Class: Artifacts for experiment 'PRE_Forecasting_Global_XGBoost' will be saved under 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../run_outputs\\PRE_Forecasting_Global_XGBoost' and 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../models_saved\\PRE_Forecasting_Global_XGBoost'\n",
      "Starting the XGBoost global pipeline execution...\n",
      "\n",
      "--- Starting Pipeline Run: Experiment 'PRE_Forecasting_Global_XGBoost' ---\n",
      "Pipeline: Loading and splitting data...\n",
      "Pipeline Error: Data file not found at constructed absolute path: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\gs:\\weatherbench2\\datasets\\era5\\1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\n",
      "Pipeline Halted: Failed at data loading/splitting.\n",
      "Pipeline execution completed. Results: Failed: Data Load/Split\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the XGBoost global pipeline for PRE...\")\n",
    "\n",
    "config_file_for_pipeline = \"../config/xgb_global/config_XGBoostGlobal_PRE.yaml\"\n",
    "pipeline = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "\n",
    "print(\"Starting the XGBoost global pipeline execution...\")\n",
    "# This single call now handles everything, including generating and saving the full predictions.\n",
    "results = pipeline.run_full_pipeline(tune=True) \n",
    "print(\"Pipeline execution completed. Results:\", results)\n",
    "\n",
    "# You can load the saved predictions file if you need to inspect it.\n",
    "# The path is defined in your config under results.predictions_filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the XGBoost global pipeline for PET...\n",
      "Pipeline Class: Attempting to load config from: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_PET.yaml\n",
      "Configuration loaded from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_PET.yaml\n",
      "Pipeline Class: Artifacts for experiment 'PET_Forecasting_Global_XGBoost' will be saved under 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../run_outputs\\PET_Forecasting_Global_XGBoost' and 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../models_saved\\PET_Forecasting_Global_XGBoost'\n",
      "Starting the XGBoost global pipeline execution for PET...\n",
      "\n",
      "--- Starting Pipeline Run: Experiment 'PRE_Forecasting_Global_XGBoost' ---\n",
      "Pipeline: Loading and splitting data...\n",
      "Pipeline Error: Data file not found at constructed absolute path: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\gs:\\weatherbench2\\datasets\\era5\\1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\n",
      "Pipeline Halted: Failed at data loading/splitting.\n",
      "Pipeline execution for PET completed. Results: Failed: Data Load/Split\n",
      "Generating predictions on the full dataset for PET...\n",
      "Pipeline: Generating predictions on the full raw dataset...\n",
      "Pipeline Error: Model or scaler not available. Cannot make full data predictions.\n",
      "Full dataset predictions for PET generated. Sample output:\n",
      "No predictions generated.\n",
      "No predictions generated.\n",
      "XGBoost global pipeline execution completed for both PRE and PET.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the XGBoost global pipeline for PET...\")\n",
    "config_file_for_pipeline = \"../config/xgb_global/config_XGBoostGlobal_PET.yaml\"\n",
    "pipeline_pet = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "print(\"Starting the XGBoost global pipeline execution for PET...\")\n",
    "results_pet = pipeline.run_full_pipeline(tune=True) \n",
    "print(\"Pipeline execution for PET completed. Results:\", results_pet)\n",
    "print(\"Generating predictions on the full dataset for PET...\")\n",
    "full_predictions_pet = pipeline_pet.predict_on_full_data()\n",
    "print(\"Full dataset predictions for PET generated. Sample output:\")\n",
    "print(full_predictions_pet.head() if full_predictions_pet is not None else \"No predictions generated.\")\n",
    "print(full_predictions_pet.tail() if full_predictions_pet is not None else \"No predictions generated.\")\n",
    "\n",
    "print(\"XGBoost global pipeline execution completed for both PRE and PET.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the XGBoost global pipeline for SPEI...\n",
      "Pipeline Class: Attempting to load config from: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_SPEI.yaml\n",
      "Configuration loaded from c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\config_XGBoostGlobal_SPEI.yaml\n",
      "Pipeline Class: Artifacts for experiment 'SPEI_Forecasting_Global_XGBoost' will be saved under 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../run_outputs\\SPEI_Forecasting_Global_XGBoost' and 'c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\../../models_saved\\SPEI_Forecasting_Global_XGBoost'\n",
      "\n",
      "--- Starting Pipeline Run: Experiment 'PRE_Forecasting_Global_XGBoost' ---\n",
      "Pipeline: Loading and splitting data...\n",
      "Pipeline Error: Data file not found at constructed absolute path: c:\\Users\\peera\\Desktop\\DroughtLSTM_oneday\\config\\xgb_global\\gs:\\weatherbench2\\datasets\\era5\\1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\n",
      "Pipeline Halted: Failed at data loading/splitting.\n",
      "Pipeline execution for SPEI completed. Results: Failed: Data Load/Split\n",
      "Generating predictions on the full dataset for SPEI...\n",
      "Pipeline: Generating predictions on the full raw dataset...\n",
      "Pipeline Error: Model or scaler not available. Cannot make full data predictions.\n",
      "Full dataset predictions for SPEI generated. Sample output:\n",
      "No predictions generated.\n",
      "No predictions generated.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing the XGBoost global pipeline for SPEI...\")\n",
    "config_file_for_pipeline = \"../config/xgb_global/config_XGBoostGlobal_SPEI.yaml\"\n",
    "pipeline_spei = XGBoostGlobalPipeline(config_path=config_file_for_pipeline)\n",
    "results_spei = pipeline.run_full_pipeline(tune=True) \n",
    "print(\"Pipeline execution for SPEI completed. Results:\", results_spei)\n",
    "print(\"Generating predictions on the full dataset for SPEI...\")\n",
    "full_predictions_spei = pipeline_spei.predict_on_full_data()\n",
    "print(\"Full dataset predictions for SPEI generated. Sample output:\")\n",
    "print(full_predictions_spei.head() if full_predictions_spei is not None else \"No predictions generated.\")\n",
    "print(full_predictions_spei.tail() if full_predictions_spei is not None else \"No predictions generated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
