{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Add project root to Python path to find the 'src' directory\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Added project root to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_client = storage.Client.create_anonymous_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_zarr(\n",
    "    \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\",\n",
    "    consolidated=True,storage_options={\"token\": \"anon\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "10m_v_component_of_wind: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "10m_wind_speed: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "2m_dewpoint_temperature: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "2m_temperature: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "above_ground: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "ageostrophic_wind_speed: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "angle_of_sub_gridscale_orography: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "anisotropy_of_sub_gridscale_orography: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "boundary_layer_height: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "divergence: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "eddy_kinetic_energy: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "geopotential: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "geopotential_at_surface: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "geostrophic_wind_speed: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "high_vegetation_cover: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "integrated_vapor_transport: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "lake_cover: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "land_sea_mask: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "lapse_rate: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "leaf_area_index_high_vegetation: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "leaf_area_index_low_vegetation: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "low_vegetation_cover: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "mean_sea_level_pressure: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_surface_latent_heat_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_surface_net_long_wave_radiation_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_surface_net_short_wave_radiation_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_surface_sensible_heat_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_top_downward_short_wave_radiation_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_top_net_long_wave_radiation_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_top_net_short_wave_radiation_flux: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "mean_vertically_integrated_moisture_divergence: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "potential_vorticity: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "relative_humidity: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "sea_ice_cover: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "sea_surface_temperature: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "slope_of_sub_gridscale_orography: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "snow_depth: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "soil_type: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "specific_humidity: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "standard_deviation_of_filtered_subgrid_orography: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "standard_deviation_of_orography: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "surface_pressure: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "temperature: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "total_cloud_cover: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "total_column_vapor: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "total_column_water: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "total_column_water_vapour: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "total_precipitation_12hr: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "total_precipitation_24hr: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "total_precipitation_6hr: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "type_of_high_vegetation: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "type_of_low_vegetation: shape=(721, 1440), dims=('latitude', 'longitude')\n",
      "u_component_of_wind: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "v_component_of_wind: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "vertical_velocity: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "volumetric_soil_water_layer_1: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "volumetric_soil_water_layer_2: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "volumetric_soil_water_layer_3: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "volumetric_soil_water_layer_4: shape=(93544, 721, 1440), dims=('time', 'latitude', 'longitude')\n",
      "vorticity: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n",
      "wind_speed: shape=(93544, 13, 721, 1440), dims=('time', 'level', 'latitude', 'longitude')\n"
     ]
    }
   ],
   "source": [
    "for var_name in ds.data_vars:\n",
    "    var = ds[var_name]\n",
    "    print(f\"{var_name}: shape={var.shape}, dims={var.dims}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Attempting to load dataset from Google Cloud Storage...\n",
      "✅ Dataset loaded successfully (metadata).\n",
      "✂️ Subsetting data...\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "cannot use ``method`` argument if any indexers are slice objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 3. Select time, space, variables\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Using method='nearest' for robustness in case slice endpoints don't exactly match coordinates.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✂️ Subsetting data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 66\u001b[0m ds_subset \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvars_to_keep\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1959\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2023\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlatitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Assuming latitude is ordered from North to South (e.g., 90 to -90)\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlongitude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Ensure correct wraparound and selection\u001b[39;49;00m\n\u001b[0;32m     70\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# --- Debugging Prints (Very helpful to confirm data before saving) ---\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- ds_subset Information ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\dataset.py:3184\u001b[0m, in \u001b[0;36mDataset.sel\u001b[1;34m(self, indexers, method, tolerance, drop, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   3116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new dataset with each array indexed by tick labels\u001b[39;00m\n\u001b[0;32m   3117\u001b[0m \u001b[38;5;124;03malong the specified dimension(s).\u001b[39;00m\n\u001b[0;32m   3118\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3181\u001b[0m \n\u001b[0;32m   3182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m indexers \u001b[38;5;241m=\u001b[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3184\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_index_queries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\n\u001b[0;32m   3186\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m drop:\n\u001b[0;32m   3189\u001b[0m     no_scalar_variables \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\indexing.py:193\u001b[0m, in \u001b[0;36mmap_index_queries\u001b[1;34m(obj, indexers, method, tolerance, **indexers_kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(IndexSelResult(labels))\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    195\u001b[0m merged \u001b[38;5;241m=\u001b[39m merge_sel_results(results)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# drop dimension coordinates found in dimension indexers\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# (also drop multi-index if any)\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# (.sel() already ensures alignment)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\indexes.py:758\u001b[0m, in \u001b[0;36mPandasIndex.sel\u001b[1;34m(self, labels, method, tolerance)\u001b[0m\n\u001b[0;32m    755\u001b[0m coord_name, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(labels\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m--> 758\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[43m_query_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_dict_like(label):\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    761\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use a dict-like object for selection on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    762\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma dimension that does not have a MultiIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    763\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\xarray\\core\\indexes.py:494\u001b[0m, in \u001b[0;36m_query_slice\u001b[1;34m(index, label, coord_name, method, tolerance)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_query_slice\u001b[39m(index, label, coord_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m tolerance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 494\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    495\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use ``method`` argument if any indexers are slice objects\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    496\u001b[0m         )\n\u001b[0;32m    497\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mslice_indexer(\n\u001b[0;32m    498\u001b[0m         _sanitize_slice_element(label\u001b[38;5;241m.\u001b[39mstart),\n\u001b[0;32m    499\u001b[0m         _sanitize_slice_element(label\u001b[38;5;241m.\u001b[39mstop),\n\u001b[0;32m    500\u001b[0m         _sanitize_slice_element(label\u001b[38;5;241m.\u001b[39mstep),\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;66;03m# unlike pandas, in xarray we never want to silently convert a\u001b[39;00m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;66;03m# slice indexer into an array indexer\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot use ``method`` argument if any indexers are slice objects"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "from pathlib import Path # For checking file size locally\n",
    "variables = [\n",
    "    # TARGET VARIABLE\n",
    "    'total_precipitation_6hr',              # Our main target\n",
    "    \n",
    "    # CORE ATMOSPHERIC VARIABLES  \n",
    "    '2m_temperature',                       # Surface temperature\n",
    "    '2m_dewpoint_temperature',              # Surface moisture content\n",
    "    'surface_pressure',                     # Surface pressure\n",
    "    'mean_sea_level_pressure',              # Synoptic pressure patterns\n",
    "    \n",
    "    # WIND FIELDS\n",
    "    '10m_u_component_of_wind',              # Surface wind U\n",
    "    '10m_v_component_of_wind',              # Surface wind V\n",
    "    '10m_wind_speed',                       # Surface wind magnitude\n",
    "    'u_component_of_wind',                  # Upper-level winds (averaged over pressure levels)\n",
    "    'v_component_of_wind',                  # Upper-level winds (averaged over pressure levels)\n",
    "    \n",
    "    # MOISTURE & THERMODYNAMICS\n",
    "    'total_column_water_vapour',            # Atmospheric moisture content\n",
    "    'integrated_vapor_transport',           # Moisture transport\n",
    "    'boundary_layer_height',                # PBL structure\n",
    "    'specific_humidity',                    # Atmospheric humidity (averaged over pressure levels)\n",
    "    \n",
    "    # CLOUDS & RADIATION\n",
    "    'total_cloud_cover',                    # Cloud coverage\n",
    "    'mean_surface_net_short_wave_radiation_flux',  # Solar heating\n",
    "    'mean_surface_latent_heat_flux',        # Evaporation\n",
    "    'mean_surface_sensible_heat_flux',      # Surface heating\n",
    "    \n",
    "    # SURFACE CONDITIONS\n",
    "    'snow_depth',                           # Snow coverage\n",
    "    'sea_surface_temperature',              # SST for coastal areas\n",
    "    'volumetric_soil_water_layer_1',        # Surface soil moisture\n",
    "    \n",
    "    # ATMOSPHERIC DYNAMICS\n",
    "    'mean_vertically_integrated_moisture_divergence',  # Moisture convergence\n",
    "    'eddy_kinetic_energy',                  # Turbulence measure\n",
    "    \n",
    "]\n",
    "\n",
    "# --- IMPORTANT NOTES FOR LOCAL VS CODE ---\n",
    "# 1. No 'google.colab.auth' is needed; authentication is handled by 'gcloud auth application-default login' run in your terminal.\n",
    "# 2. No '!pip install' commands are needed in the script itself; packages are installed once into your virtual environment.\n",
    "# 3. Output will be saved directly to your local file system.\n",
    "# ---\n",
    "\n",
    "# 1. Load the public Zarr dataset anonymously\n",
    "# 'storage_options={\"token\": \"cloud\"}' tells gcsfs to use the Application Default Credentials\n",
    "# you set up with 'gcloud auth application-default login'.\n",
    "print(\"🚀 Attempting to load dataset from Google Cloud Storage...\")\n",
    "ds = xr.open_zarr(\n",
    "    \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\",\n",
    ")\n",
    "print(\"✅ Dataset loaded successfully (metadata).\")\n",
    "\n",
    "\n",
    "# 2. Define variables to keep\n",
    "vars_to_keep = variables\n",
    "\n",
    "# 3. Select time, space, variables\n",
    "# Using method='nearest' for robustness in case slice endpoints don't exactly match coordinates.\n",
    "print(\"✂️ Subsetting data...\")\n",
    "ds_subset = ds[vars_to_keep].sel(\n",
    "    time=slice(\"1959\", \"2023\"),\n",
    "    latitude=slice(75, 30), # Assuming latitude is ordered from North to South (e.g., 90 to -90)\n",
    "    longitude=slice(-25 % 360, 50 % 360), method='nearest' # Ensure correct wraparound and selection\n",
    ")\n",
    "\n",
    "# --- Debugging Prints (Very helpful to confirm data before saving) ---\n",
    "print(\"\\n--- ds_subset Information ---\")\n",
    "print(ds_subset) # This shows the data structure, dimensions, and variables\n",
    "print(\"\\nds_subset dimensions:\")\n",
    "for dim, size in ds_subset.dims.items():\n",
    "    print(f\"  {dim}: {size}\")\n",
    "\n",
    "if all(size > 0 for size in ds_subset.dims.values()):\n",
    "    print(\"\\nAll dimensions have size > 0. Proceeding with save.\")\n",
    "else:\n",
    "    print(\"\\n❌ WARNING: One or more dimensions have a size of 0. The subset is likely empty and will result in a 0-byte file.\")\n",
    "# --- End Debugging Prints ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports successful\n",
      "🚀 Starting MESA-Net Data Loading Tests\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "TEST 1: Basic WeatherBench2 Access\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.1 is exactly one major version older than the runtime version 6.31.1 at api.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\peera\\.conda\\envs\\drought_lstm_base\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully opened WeatherBench2 zarr\n",
      "   Dataset dimensions: {'time': 93544, 'latitude': 721, 'longitude': 1440, 'level': 13}\n",
      "   Available variables: 62\n",
      "   Time range: 1959-01-01T00:00:00.000000000 to 2023-01-10T18:00:00.000000000\n",
      "\n",
      "   Variable availability check:\n",
      "   ✅ total_precipitation_6hr\n",
      "   ✅ 2m_temperature\n",
      "   ✅ surface_pressure\n",
      "   ✅ mean_sea_level_pressure\n",
      "\n",
      "==================================================\n",
      "TEST 2: Variable Inspection\n",
      "==================================================\n",
      "First 20 variables and their dimensions:\n",
      "   10m_u_component_of_wind: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   10m_v_component_of_wind: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   10m_wind_speed: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   2m_dewpoint_temperature: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   2m_temperature: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   above_ground: ('time', 'level', 'latitude', 'longitude') -> (93544, 13, 721, 1440)\n",
      "   ageostrophic_wind_speed: ('time', 'level', 'latitude', 'longitude') -> (93544, 13, 721, 1440)\n",
      "   angle_of_sub_gridscale_orography: ('latitude', 'longitude') -> (721, 1440)\n",
      "   anisotropy_of_sub_gridscale_orography: ('latitude', 'longitude') -> (721, 1440)\n",
      "   boundary_layer_height: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   divergence: ('time', 'level', 'latitude', 'longitude') -> (93544, 13, 721, 1440)\n",
      "   eddy_kinetic_energy: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   geopotential: ('time', 'level', 'latitude', 'longitude') -> (93544, 13, 721, 1440)\n",
      "   geopotential_at_surface: ('latitude', 'longitude') -> (721, 1440)\n",
      "   geostrophic_wind_speed: ('time', 'level', 'latitude', 'longitude') -> (93544, 13, 721, 1440)\n",
      "   high_vegetation_cover: ('latitude', 'longitude') -> (721, 1440)\n",
      "   integrated_vapor_transport: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "   lake_cover: ('latitude', 'longitude') -> (721, 1440)\n",
      "   land_sea_mask: ('latitude', 'longitude') -> (721, 1440)\n",
      "   lapse_rate: ('time', 'level', 'latitude', 'longitude') -> (93544, 13, 721, 1440)\n",
      "\n",
      "Testing our required variables:\n",
      "   ✅ total_precipitation_6hr: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "      Data type: float32\n",
      "   ✅ 2m_temperature: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "      Data type: float32\n",
      "   ✅ surface_pressure: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "      Data type: float32\n",
      "   ✅ mean_sea_level_pressure: ('time', 'latitude', 'longitude') -> (93544, 721, 1440)\n",
      "      Data type: float32\n",
      "\n",
      "==================================================\n",
      "TEST 3: Geographic Subsetting\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_23312\\1099730145.py:61: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"   Dataset dimensions: {dict(ds.dims)}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original longitude range: 0.0 to 359.75\n",
      "Original latitude range: -90.0 to 90.0\n",
      "Europe longitude range: 0.0 to 359.75\n",
      "Europe latitude range: 30.0 to 75.0\n",
      "Europe grid shape: lat=181, lon=301\n",
      "\n",
      "==================================================\n",
      "TEST 4: Time Subsetting\n",
      "==================================================\n",
      "✅ 2023 data: 40 time steps\n",
      "   Time range: 2023-01-01T00:00:00.000000000 to 2023-01-10T18:00:00.000000000\n",
      "   Valid sequence indices: 24 out of 40\n",
      "   First valid index: 12\n",
      "   Last valid index: 35\n",
      "\n",
      "==================================================\n",
      "TEST 5: Single Sample Extraction\n",
      "==================================================\n",
      "Testing with time index: 12\n",
      "✅ Input slice shape: Frozen({'time': 12, 'latitude': 181, 'longitude': 301, 'level': 13})\n",
      "✅ Input dims: {'time': 12, 'latitude': 181, 'longitude': 301, 'level': 13}\n",
      "    time: 12\n",
      "    latitude: 181\n",
      "    longitude: 301\n",
      "    level: 13\n",
      "✅ Target slice dims: ('time', 'latitude', 'longitude')\n",
      "✅ Target slice shape: (4, 181, 301)\n",
      "    time: 4\n",
      "    latitude: 181\n",
      "    longitude: 301\n",
      "Loading data into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peera\\AppData\\Local\\Temp\\ipykernel_23312\\1099730145.py:220: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  print(f\"✅ Input dims: {dict(input_slice.dims)}\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script for MESA-Net data loading pipeline\n",
    "Tests WeatherBench2Dataset and identifies issues\n",
    "\n",
    "Run this first to see what breaks in our data pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Add your mesa_net package to path if needed\n",
    "# sys.path.append('/path/to/your/mesa_net/')\n",
    "\n",
    "# Import your classes (adjust imports based on your file structure)\n",
    "try:\n",
    "    from src.mesanet.mesanet_dataset import WeatherBench2Dataset\n",
    "    from src.mesanet.mesanet_datamanager import WeatherBench2DataManager\n",
    "    print(\"✅ Imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please adjust the import paths based on your file structure\")\n",
    "    sys.exit(1)\n",
    "\n",
    "class DataLoadingTester:\n",
    "    \"\"\"Test data loading step by step\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.zarr_path = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "        \n",
    "        # Start with minimal variables to test\n",
    "        self.test_variables = variables\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def test_1_basic_zarr_access(self):\n",
    "        \"\"\"Test 1: Can we access WeatherBench2 zarr directly?\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 1: Basic WeatherBench2 Access\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Try to open the zarr store\n",
    "            ds = xr.open_zarr(\n",
    "                self.zarr_path,\n",
    "                consolidated=True,\n",
    "                storage_options={\"token\": \"anon\"},\n",
    "                chunks={'time': 10}\n",
    "            )\n",
    "            \n",
    "            print(\"✅ Successfully opened WeatherBench2 zarr\")\n",
    "            print(f\"   Dataset dimensions: {dict(ds.dims)}\")\n",
    "            print(f\"   Available variables: {len(list(ds.data_vars.keys()))}\")\n",
    "            print(f\"   Time range: {ds.time.values[0]} to {ds.time.values[-1]}\")\n",
    "            \n",
    "            # Check which of our test variables exist\n",
    "            available_vars = list(ds.data_vars.keys())\n",
    "            print(f\"\\n   Variable availability check:\")\n",
    "            for var in self.test_variables:\n",
    "                exists = var in available_vars\n",
    "                status = \"✅\" if exists else \"❌\"\n",
    "                print(f\"   {status} {var}\")\n",
    "            \n",
    "            self.results['zarr_access'] = True\n",
    "            self.results['available_variables'] = available_vars\n",
    "            return ds\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to access WeatherBench2: {e}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['zarr_access'] = False\n",
    "            return None\n",
    "    \n",
    "    def test_2_variable_inspection(self, ds):\n",
    "        \"\"\"Test 2: Inspect variables in detail\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 2: Variable Inspection\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if ds is None:\n",
    "            print(\"❌ Skipping - no dataset available\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Print first 20 variables with their dimensions\n",
    "            variables = list(ds.data_vars.keys())[:20]\n",
    "            print(f\"First 20 variables and their dimensions:\")\n",
    "            \n",
    "            for var in variables:\n",
    "                dims = ds[var].dims\n",
    "                shape = ds[var].shape\n",
    "                print(f\"   {var}: {dims} -> {shape}\")\n",
    "            \n",
    "            # Test specific variables we need\n",
    "            print(f\"\\nTesting our required variables:\")\n",
    "            for var in self.test_variables:\n",
    "                if var in ds.data_vars:\n",
    "                    var_data = ds[var]\n",
    "                    print(f\"   ✅ {var}: {var_data.dims} -> {var_data.shape}\")\n",
    "                    print(f\"      Data type: {var_data.dtype}\")\n",
    "                    #print(f\"      Min/Max: {float(var_data.min())} / {float(var_data.max())}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ {var}: NOT FOUND\")\n",
    "            \n",
    "            self.results['variable_inspection'] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Variable inspection failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['variable_inspection'] = False\n",
    "    \n",
    "    def test_3_geographic_subsetting(self, ds):\n",
    "        \"\"\"Test 3: Geographic subsetting for Europe\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 3: Geographic Subsetting\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if ds is None:\n",
    "            print(\"❌ Skipping - no dataset available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            print(f\"Original longitude range: {ds.longitude.min().values} to {ds.longitude.max().values}\")\n",
    "            print(f\"Original latitude range: {ds.latitude.min().values} to {ds.latitude.max().values}\")\n",
    "            \n",
    "            # Apply Europe bounds\n",
    "            ds_europe = ds.where(\n",
    "                (ds.longitude >= 335) | (ds.longitude <= 50),\n",
    "                drop=True\n",
    "            ).sel(latitude=slice(75, 30))\n",
    "            \n",
    "            print(f\"Europe longitude range: {ds_europe.longitude.min().values} to {ds_europe.longitude.max().values}\")\n",
    "            print(f\"Europe latitude range: {ds_europe.latitude.min().values} to {ds_europe.latitude.max().values}\")\n",
    "            print(f\"Europe grid shape: lat={len(ds_europe.latitude)}, lon={len(ds_europe.longitude)}\")\n",
    "            \n",
    "            self.results['geographic_subsetting'] = True\n",
    "            return ds_europe\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Geographic subsetting failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['geographic_subsetting'] = False\n",
    "            return None\n",
    "    \n",
    "    def test_4_time_subsetting(self, ds):\n",
    "        \"\"\"Test 4: Time subsetting and indexing\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 4: Time Subsetting\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if ds is None:\n",
    "            print(\"❌ Skipping - no dataset available\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Test recent time period\n",
    "            ds_recent = ds.sel(time=slice(\"2023\", \"2023\"))\n",
    "            print(f\"✅ 2023 data: {len(ds_recent.time)} time steps\")\n",
    "            print(f\"   Time range: {ds_recent.time.values[0]} to {ds_recent.time.values[-1]}\")\n",
    "            \n",
    "            # Test sequence creation indices\n",
    "            total_time_steps = len(ds_recent.time)\n",
    "            sequence_length = 12\n",
    "            forecast_horizon = 4\n",
    "            \n",
    "            valid_indices = np.arange(\n",
    "                sequence_length,\n",
    "                total_time_steps - forecast_horizon\n",
    "            )\n",
    "            \n",
    "            print(f\"   Valid sequence indices: {len(valid_indices)} out of {total_time_steps}\")\n",
    "            \n",
    "            if len(valid_indices) > 0:\n",
    "                print(f\"   First valid index: {valid_indices[0]}\")\n",
    "                print(f\"   Last valid index: {valid_indices[-1]}\")\n",
    "            else:\n",
    "                print(\"   ❌ No valid sequences found!\")\n",
    "            \n",
    "            self.results['time_subsetting'] = True\n",
    "            return ds_recent, valid_indices\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Time subsetting failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['time_subsetting'] = False\n",
    "            return None, None\n",
    "    \n",
    "    def test_5_single_sample_extraction(self, ds, valid_indices):\n",
    "        \"\"\"Test 5: Extract a single training sample\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 5: Single Sample Extraction\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if ds is None or len(valid_indices) == 0:\n",
    "            print(\"❌ Skipping - no dataset or valid indices available\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            sequence_length = 12\n",
    "            forecast_horizon = 4\n",
    "            time_idx = valid_indices[0]\n",
    "            \n",
    "            print(f\"Testing with time index: {time_idx}\")\n",
    "            \n",
    "            # Input sequence\n",
    "            input_slice = ds.isel(\n",
    "                time=slice(time_idx - sequence_length, time_idx)\n",
    "            )\n",
    "            print(f\"✅ Input slice shape: {input_slice.sizes}\")\n",
    "            print(f\"✅ Input dims: {dict(input_slice.dims)}\")\n",
    "\n",
    "            for dim, size in input_slice.sizes.items():\n",
    "                print(f\"    {dim}: {size}\")\n",
    "            \n",
    "            # Target sequence  \n",
    "            if 'total_precipitation_6hr' in ds.data_vars:\n",
    "                target_slice = ds['total_precipitation_6hr'].isel(\n",
    "                    time=slice(time_idx, time_idx + forecast_horizon)\n",
    "                )\n",
    "                print(f\"✅ Target slice dims: {target_slice.dims}\")\n",
    "                print(f\"✅ Target slice shape: {target_slice.shape}\")\n",
    "                for dim, size in target_slice.sizes.items():\n",
    "                    print(f\"    {dim}: {size}\")\n",
    "            else:\n",
    "                print(\"❌ total_precipitation_6hr not found\")\n",
    "                return\n",
    "            \n",
    "            # Test loading the data\n",
    "            print(\"Loading data into memory...\")\n",
    "            input_loaded = input_slice.load()\n",
    "            target_loaded = target_slice.load()\n",
    "            \n",
    "            print(f\"✅ Successfully loaded sample\")\n",
    "            print(f\"   Input time steps: {len(input_loaded.time)}\")\n",
    "            print(f\"   Target time steps: {len(target_loaded.time)}\")\n",
    "            \n",
    "            self.results['sample_extraction'] = True\n",
    "            return input_loaded, target_loaded\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Sample extraction failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['sample_extraction'] = False\n",
    "            return None, None\n",
    "    \n",
    "    def test_6_tensor_conversion(self, input_data, target_data):\n",
    "        \"\"\"Test 6: Convert xarray to PyTorch tensors\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 6: Tensor Conversion\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        if input_data is None or target_data is None:\n",
    "            print(\"❌ Skipping - no data available\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Test basic tensor conversion for target (single variable)\n",
    "            print(\"Testing target tensor conversion...\")\n",
    "            target_array = target_data.values\n",
    "            target_tensor = torch.tensor(target_array, dtype=torch.float32)\n",
    "            print(f\"✅ Target tensor shape: {target_tensor.shape}\")\n",
    "            print(f\"   Data type: {target_tensor.dtype}\")\n",
    "            print(f\"   Value range: {target_tensor.min().item():.4f} to {target_tensor.max().item():.4f}\")\n",
    "            \n",
    "            # Test multi-variable input conversion (this is where we'll likely have issues)\n",
    "            print(\"\\nTesting input tensor conversion...\")\n",
    "            var_arrays = []\n",
    "            \n",
    "            for var in self.test_variables:\n",
    "                if var in input_data.data_vars:\n",
    "                    var_data = input_data[var].values\n",
    "                    print(f\"   Processing {var}: shape {var_data.shape}\")\n",
    "                    \n",
    "                    # Handle different dimensionalities\n",
    "                    if var_data.ndim == 2:  # (lat, lon) - single time step\n",
    "                        var_data = var_data[None, ...]  # Add time dimension\n",
    "                    elif var_data.ndim == 4:  # (time, level, lat, lon) - multi-level\n",
    "                        # Average over pressure levels\n",
    "                        var_data = np.mean(var_data, axis=1)\n",
    "                        print(f\"     Averaged over pressure levels: {var_data.shape}\")\n",
    "                    \n",
    "                    var_arrays.append(var_data)\n",
    "                    print(f\"   ✅ {var}: final shape {var_data.shape}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ {var}: not found in data\")\n",
    "            \n",
    "            if var_arrays:\n",
    "                # Stack variables\n",
    "                stacked_array = np.stack(var_arrays, axis=1)\n",
    "                input_tensor = torch.tensor(stacked_array, dtype=torch.float32)\n",
    "                \n",
    "                print(f\"✅ Input tensor shape: {input_tensor.shape}\")\n",
    "                print(f\"   Expected format: (time, vars, lat, lon)\")\n",
    "                print(f\"   Data type: {input_tensor.dtype}\")\n",
    "                \n",
    "                self.results['tensor_conversion'] = True\n",
    "                return input_tensor, target_tensor\n",
    "            else:\n",
    "                print(\"❌ No variables could be processed\")\n",
    "                self.results['tensor_conversion'] = False\n",
    "                return None, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Tensor conversion failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['tensor_conversion'] = False\n",
    "            return None, None\n",
    "    \n",
    "    def test_7_dataset_class(self):\n",
    "        \"\"\"Test 7: Our WeatherBench2Dataset class\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"TEST 7: WeatherBench2Dataset Class\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Create dataset with minimal configuration\n",
    "            dataset = WeatherBench2Dataset(\n",
    "                zarr_path=self.zarr_path,\n",
    "                variables=self.test_variables,\n",
    "                time_range=slice(\"2023\", \"2023\"),  # Just 2023 for testing\n",
    "                split=\"train\",\n",
    "                sequence_length=12,\n",
    "                forecast_horizon=4,\n",
    "                normalize=False  # Skip normalization for now\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Dataset created successfully\")\n",
    "            print(f\"   Dataset length: {len(dataset)}\")\n",
    "            \n",
    "            # Test getting one sample\n",
    "            if len(dataset) > 0:\n",
    "                print(\"Testing sample retrieval...\")\n",
    "                input_seq, target_seq, geo_features = dataset[0]\n",
    "                \n",
    "                print(f\"✅ Sample retrieved successfully\")\n",
    "                print(f\"   Input sequence shape: {input_seq.shape}\")\n",
    "                print(f\"   Target sequence shape: {target_seq.shape}\")\n",
    "                print(f\"   Geographic features shape: {geo_features.shape}\")\n",
    "                \n",
    "                self.results['dataset_class'] = True\n",
    "            else:\n",
    "                print(\"❌ Dataset is empty\")\n",
    "                self.results['dataset_class'] = False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Dataset class test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['dataset_class'] = False\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run all tests in sequence\"\"\"\n",
    "        print(\"🚀 Starting MESA-Net Data Loading Tests\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Test 1: Basic access\n",
    "        ds = self.test_1_basic_zarr_access()\n",
    "        \n",
    "        # Test 2: Variable inspection\n",
    "        self.test_2_variable_inspection(ds)\n",
    "        \n",
    "        # Test 3: Geographic subsetting\n",
    "        ds_europe = self.test_3_geographic_subsetting(ds)\n",
    "        \n",
    "        # Test 4: Time subsetting\n",
    "        ds_recent, valid_indices = self.test_4_time_subsetting(ds_europe)\n",
    "        \n",
    "        # Test 5: Sample extraction\n",
    "        input_data, target_data = self.test_5_single_sample_extraction(ds_recent, valid_indices)\n",
    "        \n",
    "        # Test 6: Tensor conversion\n",
    "        input_tensor, target_tensor = self.test_6_tensor_conversion(input_data, target_data)\n",
    "        \n",
    "        # Test 7: Dataset class\n",
    "        self.test_7_dataset_class()\n",
    "        \n",
    "        # Summary\n",
    "        self.print_summary()\n",
    "        return self.print_summary()  # ✅ Add this line\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print test results summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🏁 TEST RESULTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        tests = [\n",
    "            ('zarr_access', 'WeatherBench2 Access'),\n",
    "            ('variable_inspection', 'Variable Inspection'),\n",
    "            ('geographic_subsetting', 'Geographic Subsetting'),\n",
    "            ('time_subsetting', 'Time Subsetting'),\n",
    "            ('sample_extraction', 'Sample Extraction'),\n",
    "            ('tensor_conversion', 'Tensor Conversion'),\n",
    "            ('dataset_class', 'Dataset Class'),\n",
    "        ]\n",
    "        \n",
    "        passed = 0\n",
    "        total = len(tests)\n",
    "        \n",
    "        for test_key, test_name in tests:\n",
    "            if test_key in self.results:\n",
    "                status = \"✅ PASS\" if self.results[test_key] else \"❌ FAIL\"\n",
    "                if self.results[test_key]:\n",
    "                    passed += 1\n",
    "            else:\n",
    "                status = \"⏭️ SKIP\"\n",
    "            \n",
    "            print(f\"{status} {test_name}\")\n",
    "        \n",
    "        print(f\"\\nOverall: {passed}/{total} tests passed\")\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"🎉 All tests passed! Data pipeline is working.\")\n",
    "        else:\n",
    "            print(\"🔧 Some tests failed. Check the errors above and fix issues.\")\n",
    "            \n",
    "        return passed == total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tester = DataLoadingTester()\n",
    "    success = tester.run_all_tests()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✅ Ready to proceed to next step!\")\n",
    "    else:\n",
    "        print(\"\\n❌ Fix data loading issues before proceeding.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive test for fixed MESA-Net data loading pipeline\n",
    "Tests all edge cases and error handling\n",
    "\n",
    "File: test_fixed_data_pipeline.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import traceback\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import your fixed classes\n",
    "try:\n",
    "    # Update these imports based on your file structure\n",
    "    from src.mesanet.mesanet_dataset import WeatherBench2Dataset, WeatherBench2DataManager\n",
    "    print(\"✅ Fixed imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"Please adjust the import paths and ensure you've updated the dataset file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "class ComprehensiveDataTester:\n",
    "    \"\"\"Comprehensive test for fixed data pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.zarr_path = \"gs://weatherbench2/datasets/era5/1959-2023_01_10-wb13-6h-1440x721_with_derived_variables.zarr\"\n",
    "        \n",
    "        # Test with realistic variable list\n",
    "        self.test_variables = [\n",
    "            'total_precipitation_6hr',\n",
    "            '2m_temperature',\n",
    "            'surface_pressure', \n",
    "            'mean_sea_level_pressure',\n",
    "            '10m_u_component_of_wind',\n",
    "            '10m_v_component_of_wind'\n",
    "        ]\n",
    "        \n",
    "        # Also test with some invalid variables\n",
    "        self.test_variables_with_invalid = self.test_variables + [\n",
    "            'fake_variable_1',\n",
    "            'non_existent_var',\n",
    "            'invalid_precipitation'\n",
    "        ]\n",
    "        \n",
    "        self.results = {}\n",
    "    \n",
    "    def test_1_data_manager_connection(self):\n",
    "        \"\"\"Test 1: Data manager connection and variable validation\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 1: Data Manager Connection & Variable Validation\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Test with valid variables\n",
    "            print(\"Testing with valid variables...\")\n",
    "            data_manager = WeatherBench2DataManager(\n",
    "                zarr_path=self.zarr_path,\n",
    "                variables=self.test_variables,\n",
    "                sequence_length=12,\n",
    "                forecast_horizon=4\n",
    "            )\n",
    "            print(\"✅ Data manager created with valid variables\")\n",
    "            \n",
    "            # Test with some invalid variables\n",
    "            print(\"\\nTesting with mixed valid/invalid variables...\")\n",
    "            data_manager_mixed = WeatherBench2DataManager(\n",
    "                zarr_path=self.zarr_path,\n",
    "                variables=self.test_variables_with_invalid,\n",
    "                sequence_length=12,\n",
    "                forecast_horizon=4\n",
    "            )\n",
    "            print(\"✅ Data manager handles invalid variables gracefully\")\n",
    "            \n",
    "            self.data_manager = data_manager\n",
    "            self.results['data_manager'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Data manager test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['data_manager'] = False\n",
    "            return False\n",
    "    \n",
    "    def test_2_dataset_creation_validation(self):\n",
    "        \"\"\"Test 2: Dataset creation with variable validation\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 2: Dataset Creation & Variable Validation\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.results.get('data_manager', False):\n",
    "            print(\"❌ Skipping - data manager test failed\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Create dataset with mixed variables\n",
    "            dataset = WeatherBench2Dataset(\n",
    "                zarr_path=self.zarr_path,\n",
    "                variables=self.test_variables_with_invalid,\n",
    "                time_range=slice(\"2023\", \"2023\"),\n",
    "                split=\"train\",\n",
    "                sequence_length=12,\n",
    "                forecast_horizon=4,\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Dataset created successfully\")\n",
    "            print(f\"   Original variables requested: {len(self.test_variables_with_invalid)}\")\n",
    "            print(f\"   Valid variables found: {len(dataset.available_variables)}\")\n",
    "            print(f\"   Available variables: {dataset.available_variables}\")\n",
    "            print(f\"   Dataset length: {len(dataset)}\")\n",
    "            \n",
    "            # Get variable info\n",
    "            info = dataset.get_variable_info()\n",
    "            print(f\"\\n   Dataset info:\")\n",
    "            print(f\"   - Shape: {info['dataset_shape']}\")\n",
    "            print(f\"   - Geo features shape: {info['geographic_features_shape']}\")\n",
    "            print(f\"   - Normalization stats computed: {len(info['normalization_stats'])}\")\n",
    "            \n",
    "            self.dataset = dataset\n",
    "            self.results['dataset_creation'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Dataset creation test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['dataset_creation'] = False\n",
    "            return False\n",
    "    \n",
    "    def test_3_single_sample_retrieval(self):\n",
    "        \"\"\"Test 3: Single sample retrieval and tensor shapes\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 3: Single Sample Retrieval & Tensor Shapes\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.results.get('dataset_creation', False):\n",
    "            print(\"❌ Skipping - dataset creation failed\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            if len(self.dataset) == 0:\n",
    "                print(\"❌ Dataset is empty - cannot test sample retrieval\")\n",
    "                self.results['sample_retrieval'] = False\n",
    "                return False\n",
    "            \n",
    "            # Test multiple samples\n",
    "            for i in [0, len(self.dataset)//2, len(self.dataset)-1]:\n",
    "                print(f\"\\nTesting sample {i}...\")\n",
    "                \n",
    "                input_seq, target_seq, geo_features = self.dataset[i]\n",
    "                \n",
    "                print(f\"✅ Sample {i} retrieved successfully\")\n",
    "                print(f\"   Input sequence shape: {input_seq.shape}\")\n",
    "                print(f\"   Target sequence shape: {target_seq.shape}\")\n",
    "                print(f\"   Geographic features shape: {geo_features.shape}\")\n",
    "                \n",
    "                # Validate tensor properties\n",
    "                assert input_seq.dtype == torch.float32, f\"Wrong input dtype: {input_seq.dtype}\"\n",
    "                assert target_seq.dtype == torch.float32, f\"Wrong target dtype: {target_seq.dtype}\"\n",
    "                assert geo_features.dtype == torch.float32, f\"Wrong geo dtype: {geo_features.dtype}\"\n",
    "                \n",
    "                # Check for NaN values\n",
    "                assert not torch.isnan(input_seq).any(), \"Input contains NaN values\"\n",
    "                assert not torch.isnan(target_seq).any(), \"Target contains NaN values\"\n",
    "                assert not torch.isnan(geo_features).any(), \"Geo features contain NaN values\"\n",
    "                \n",
    "                # Check value ranges are reasonable\n",
    "                print(f\"   Input value range: {input_seq.min():.4f} to {input_seq.max():.4f}\")\n",
    "                print(f\"   Target value range: {target_seq.min():.4f} to {target_seq.max():.4f}\")\n",
    "                print(f\"   Geo features range: {geo_features.min():.4f} to {geo_features.max():.4f}\")\n",
    "            \n",
    "            print(\"✅ All sample retrievals successful\")\n",
    "            self.results['sample_retrieval'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Sample retrieval test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['sample_retrieval'] = False\n",
    "            return False\n",
    "    \n",
    "    def test_4_dataloader_functionality(self):\n",
    "        \"\"\"Test 4: DataLoader functionality with error handling\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 4: DataLoader Functionality & Error Handling\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.results.get('dataset_creation', False):\n",
    "            print(\"❌ Skipping - dataset creation failed\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Create datasets\n",
    "            train_dataset, val_dataset, test_dataset = self.data_manager.create_datasets(\n",
    "                time_range=slice(\"2023\", \"2023\"),\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ Datasets created:\")\n",
    "            print(f\"   Train: {len(train_dataset)} samples\")\n",
    "            print(f\"   Val: {len(val_dataset)} samples\")\n",
    "            print(f\"   Test: {len(test_dataset)} samples\")\n",
    "            \n",
    "            # Create data loaders\n",
    "            train_loader, val_loader, test_loader = self.data_manager.create_data_loaders(\n",
    "                datasets=(train_dataset, val_dataset, test_dataset),\n",
    "                batch_size=4,  # Small batch for testing\n",
    "                num_workers=0  # No multiprocessing for testing\n",
    "            )\n",
    "            \n",
    "            print(f\"✅ DataLoaders created:\")\n",
    "            print(f\"   Train batches: {len(train_loader)}\")\n",
    "            print(f\"   Val batches: {len(val_loader)}\")\n",
    "            print(f\"   Test batches: {len(test_loader)}\")\n",
    "            \n",
    "            # Test loading batches\n",
    "            print(f\"\\nTesting batch loading...\")\n",
    "            \n",
    "            for loader_name, loader in [(\"Train\", train_loader), (\"Val\", val_loader)]:\n",
    "                if len(loader) > 0:\n",
    "                    batch_input, batch_target, batch_geo = next(iter(loader))\n",
    "                    \n",
    "                    print(f\"✅ {loader_name} batch loaded:\")\n",
    "                    print(f\"   Batch input shape: {batch_input.shape}\")\n",
    "                    print(f\"   Batch target shape: {batch_target.shape}\")\n",
    "                    print(f\"   Batch geo shape: {batch_geo.shape}\")\n",
    "                    \n",
    "                    # Validate batch properties\n",
    "                    assert batch_input.dtype == torch.float32\n",
    "                    assert batch_target.dtype == torch.float32\n",
    "                    assert batch_geo.dtype == torch.float32\n",
    "                    \n",
    "                    # Check for NaN values\n",
    "                    assert not torch.isnan(batch_input).any()\n",
    "                    assert not torch.isnan(batch_target).any()\n",
    "                    assert not torch.isnan(batch_geo).any()\n",
    "                    \n",
    "                    print(f\"   ✅ Batch validation passed\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ {loader_name} loader is empty\")\n",
    "            \n",
    "            self.train_loader = train_loader\n",
    "            self.results['dataloader'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ DataLoader test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['dataloader'] = False\n",
    "            return False\n",
    "    \n",
    "    def test_5_normalization_effectiveness(self):\n",
    "        \"\"\"Test 5: Normalization effectiveness\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 5: Normalization Effectiveness\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.results.get('dataloader', False):\n",
    "            print(\"❌ Skipping - dataloader test failed\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Test with normalization\n",
    "            print(\"Testing WITH normalization...\")\n",
    "            dataset_norm = WeatherBench2Dataset(\n",
    "                zarr_path=self.zarr_path,\n",
    "                variables=self.test_variables[:3],  # Use first 3 variables\n",
    "                time_range=slice(\"2023\", \"2023\"),\n",
    "                split=\"train\",\n",
    "                sequence_length=12,\n",
    "                forecast_horizon=4,\n",
    "                normalize=True\n",
    "            )\n",
    "            \n",
    "            # Test without normalization\n",
    "            print(\"Testing WITHOUT normalization...\")\n",
    "            dataset_no_norm = WeatherBench2Dataset(\n",
    "                zarr_path=self.zarr_path,\n",
    "                variables=self.test_variables[:3],  # Use first 3 variables\n",
    "                time_range=slice(\"2023\", \"2023\"),\n",
    "                split=\"train\",\n",
    "                sequence_length=12,\n",
    "                forecast_horizon=4,\n",
    "                normalize=False\n",
    "            )\n",
    "            \n",
    "            if len(dataset_norm) > 0 and len(dataset_no_norm) > 0:\n",
    "                # Get sample from each\n",
    "                input_norm, _, _ = dataset_norm[0]\n",
    "                input_no_norm, _, _ = dataset_no_norm[0]\n",
    "                \n",
    "                print(f\"\\nNormalization comparison:\")\n",
    "                print(f\"   Normalized - Mean: {input_norm.mean():.4f}, Std: {input_norm.std():.4f}\")\n",
    "                print(f\"   Not normalized - Mean: {input_no_norm.mean():.4f}, Std: {input_no_norm.std():.4f}\")\n",
    "                print(f\"   Normalized range: {input_norm.min():.4f} to {input_norm.max():.4f}\")\n",
    "                print(f\"   Not normalized range: {input_no_norm.min():.4f} to {input_no_norm.max():.4f}\")\n",
    "                \n",
    "                # Check that normalization actually changed the data\n",
    "                if not torch.allclose(input_norm, input_no_norm, atol=1e-3):\n",
    "                    print(\"✅ Normalization is working correctly\")\n",
    "                else:\n",
    "                    print(\"⚠️ Normalization may not be applied correctly\")\n",
    "            \n",
    "            self.results['normalization'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Normalization test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['normalization'] = False\n",
    "            return False\n",
    "    \n",
    "    def test_6_memory_efficiency(self):\n",
    "        \"\"\"Test 6: Memory efficiency and batch processing\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 6: Memory Efficiency & Batch Processing\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.results.get('dataloader', False):\n",
    "            print(\"❌ Skipping - dataloader test failed\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Test processing multiple batches\n",
    "            print(\"Testing multiple batch processing...\")\n",
    "            \n",
    "            batch_count = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            for batch_idx, (input_batch, target_batch, geo_batch) in enumerate(self.train_loader):\n",
    "                batch_count += 1\n",
    "                total_samples += input_batch.shape[0]\n",
    "                \n",
    "                print(f\"   Batch {batch_idx}: {input_batch.shape[0]} samples\")\n",
    "                \n",
    "                # Test memory usage\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"   GPU memory: {torch.cuda.memory_allocated()/1e6:.1f}MB\")\n",
    "                \n",
    "                # Limit test to first few batches\n",
    "                if batch_count >= 3:\n",
    "                    break\n",
    "            \n",
    "            print(f\"✅ Processed {batch_count} batches, {total_samples} total samples\")\n",
    "            \n",
    "            # Test iterator restart\n",
    "            print(\"\\nTesting DataLoader iterator restart...\")\n",
    "            batch_count_2 = sum(1 for _ in self.train_loader)\n",
    "            print(f\"✅ Second iteration: {batch_count_2} batches\")\n",
    "            \n",
    "            self.results['memory_efficiency'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Memory efficiency test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['memory_efficiency'] = False\n",
    "            return False\n",
    "    \n",
    "    def test_7_edge_cases(self):\n",
    "        \"\"\"Test 7: Edge cases and error handling\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST 7: Edge Cases & Error Handling\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            print(\"Testing edge cases...\")\n",
    "            \n",
    "            # Test with empty variable list\n",
    "            print(\"\\n1. Testing with empty variable list...\")\n",
    "            try:\n",
    "                empty_dataset = WeatherBench2Dataset(\n",
    "                    zarr_path=self.zarr_path,\n",
    "                    variables=[],\n",
    "                    time_range=slice(\"2023\", \"2023\"),\n",
    "                    split=\"train\",\n",
    "                    sequence_length=12,\n",
    "                    forecast_horizon=4,\n",
    "                    normalize=False\n",
    "                )\n",
    "                print(\"❌ Should have failed with empty variables\")\n",
    "            except Exception:\n",
    "                print(\"✅ Correctly handled empty variable list\")\n",
    "            \n",
    "            # Test with very small time range\n",
    "            print(\"\\n2. Testing with minimal time range...\")\n",
    "            try:\n",
    "                minimal_dataset = WeatherBench2Dataset(\n",
    "                    zarr_path=self.zarr_path,\n",
    "                    variables=self.test_variables[:2],\n",
    "                    time_range=slice(\"2023-01-01\", \"2023-01-02\"),\n",
    "                    split=\"train\",\n",
    "                    sequence_length=12,\n",
    "                    forecast_horizon=4,\n",
    "                    normalize=False\n",
    "                )\n",
    "                print(f\"✅ Minimal dataset created: {len(minimal_dataset)} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Minimal time range failed: {e}\")\n",
    "            \n",
    "            # Test with large sequence length\n",
    "            print(\"\\n3. Testing with large sequence length...\")\n",
    "            try:\n",
    "                large_seq_dataset = WeatherBench2Dataset(\n",
    "                    zarr_path=self.zarr_path,\n",
    "                    variables=self.test_variables[:2],\n",
    "                    time_range=slice(\"2023\", \"2023\"),\n",
    "                    split=\"train\",\n",
    "                    sequence_length=50,  # Very large\n",
    "                    forecast_horizon=4,\n",
    "                    normalize=False\n",
    "                )\n",
    "                print(f\"✅ Large sequence dataset: {len(large_seq_dataset)} samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Large sequence failed: {e}\")\n",
    "            \n",
    "            # Test invalid time range\n",
    "            print(\"\\n4. Testing with invalid time range...\")\n",
    "            try:\n",
    "                invalid_dataset = WeatherBench2Dataset(\n",
    "                    zarr_path=self.zarr_path,\n",
    "                    variables=self.test_variables[:2],\n",
    "                    time_range=slice(\"2030\", \"2031\"),  # Future dates\n",
    "                    split=\"train\",\n",
    "                    sequence_length=12,\n",
    "                    forecast_horizon=4,\n",
    "                    normalize=False\n",
    "                )\n",
    "                print(f\"⚠️ Invalid time range accepted: {len(invalid_dataset)} samples\")\n",
    "            except Exception:\n",
    "                print(\"✅ Correctly handled invalid time range\")\n",
    "            \n",
    "            self.results['edge_cases'] = True\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Edge cases test failed: {e}\")\n",
    "            traceback.print_exc()\n",
    "            self.results['edge_cases'] = False\n",
    "            return False\n",
    "    \n",
    "    def run_all_tests(self):\n",
    "        \"\"\"Run all comprehensive tests\"\"\"\n",
    "        print(\"🚀 Starting Comprehensive MESA-Net Data Pipeline Tests\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Run all tests\n",
    "        self.test_1_data_manager_connection()\n",
    "        self.test_2_dataset_creation_validation()\n",
    "        self.test_3_single_sample_retrieval()\n",
    "        self.test_4_dataloader_functionality()\n",
    "        self.test_5_normalization_effectiveness()\n",
    "        self.test_6_memory_efficiency()\n",
    "        self.test_7_edge_cases()\n",
    "        \n",
    "        # Print summary\n",
    "        return self.print_summary()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print comprehensive test results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"🏁 COMPREHENSIVE DATA PIPELINE TEST RESULTS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        tests = [\n",
    "            ('data_manager', 'Data Manager Connection'),\n",
    "            ('dataset_creation', 'Dataset Creation & Validation'),\n",
    "            ('sample_retrieval', 'Single Sample Retrieval'),\n",
    "            ('dataloader', 'DataLoader Functionality'),\n",
    "            ('normalization', 'Normalization Effectiveness'),\n",
    "            ('memory_efficiency', 'Memory Efficiency'),\n",
    "            ('edge_cases', 'Edge Cases & Error Handling'),\n",
    "        ]\n",
    "        \n",
    "        passed = 0\n",
    "        total = len(tests)\n",
    "        \n",
    "        for test_key, test_name in tests:\n",
    "            if test_key in self.results:\n",
    "                status = \"✅ PASS\" if self.results[test_key] else \"❌ FAIL\"\n",
    "                if self.results[test_key]:\n",
    "                    passed += 1\n",
    "            else:\n",
    "                status = \"⏭️ SKIP\"\n",
    "            \n",
    "            print(f\"{status} {test_name}\")\n",
    "        \n",
    "        print(f\"\\nOverall: {passed}/{total} tests passed\")\n",
    "        \n",
    "        if passed == total:\n",
    "            print(\"🎉 ALL COMPREHENSIVE TESTS PASSED!\")\n",
    "            print(\"✅ Data pipeline is robust and ready for production\")\n",
    "            print(\"✅ Ready to proceed to model testing\")\n",
    "        elif passed >= total * 0.8:\n",
    "            print(\"⚠️ Most tests passed - minor issues to fix\")\n",
    "            print(\"✅ Can proceed with caution\")\n",
    "        else:\n",
    "            print(\"❌ Major issues found - need to fix before proceeding\")\n",
    "            \n",
    "        return passed >= total * 0.8  # 80% pass rate required\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE MESA-NET DATA PIPELINE TEST\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"This test validates the fixed data loading implementation\")\n",
    "    print(\"It checks error handling, edge cases, and production readiness\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    tester = ComprehensiveDataTester()\n",
    "    success = tester.run_all_tests()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 DATA PIPELINE IS PRODUCTION READY!\")\n",
    "        print(\"✅ All critical functionality working\")\n",
    "        print(\"✅ Error handling robust\")\n",
    "        print(\"✅ Ready for Step 2: Basic Model Testing\")\n",
    "    else:\n",
    "        print(\"\\n❌ DATA PIPELINE NEEDS FIXES\")\n",
    "        print(\"Please review the failed tests above\")\n",
    "        print(\"Fix issues before proceeding to model testing\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drought_lstm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
